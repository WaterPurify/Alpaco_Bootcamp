{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2gxi52SpPbzzUt3IgMorT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaterPurify/Text_Mining/blob/main/Textmining_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##소개\n",
        "\n",
        "토근화는 NLP 파이프라인에서 첫번째 스텝입니다. Raw 문자가 작은 덩어리의 단어들과 문장들로 나눠지는 것이 토큰이라고 하고 과정을 토큰화라고 불립니다.\n",
        "* Raw 문자가 '단어'들로 나눠지면 '단어 토큰화'라고 불립니다.\n",
        "* Raw 문자가 '문장'들로 나눠지면 '문장 토큰화'라고 불립니다.\n",
        "\n",
        "기본적으로 뛰어쓰기는 단어 토큰화에 사용하고 점,느낌표, 새 문단 뛰어쓰기는 문장 토근화에 사용합니다. 테스크에 따라서 적절한 방법을 골라야 한다.다만 space랑 punctuation은 무시할 수 있고 최종 토큰 리스트에 들어가지 않습니다.\n"
      ],
      "metadata": {
        "id": "1RJDKFxZJjlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##왜 토근화를 해야 되는지?\n",
        "\n",
        "모든 문자는 단어들을 통해서 의미를 얻습니다. 그래서 문자안에 있는 단어들을 분석함으로서 저희는 문자의 의미를 쉽게 해석할 수 있습니다. 그리고 단어들의 리스트를 가지고 있으면 저희는 쉽게 문자에서 insight들을 얻기 위한 통계학적인 도구들과 방법들로 사용할 수 있습니다.예를 들어, 단어 빈도와 단어수를 이용하여 문장이나 document에서 중요한 단어를 찾을 수 있습니다."
      ],
      "metadata": {
        "id": "SJJ0IKxTQzpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##토큰화 방법들\n",
        "\n",
        "토큰화를 하는 방법들이 많이 존재합니다. 언어, library와 모델의 목적에 따라서 방법을 정할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "HpWK2rrnQ6Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "파이썬의 built-in 방법을 통해서 토큰화\n",
        "\n",
        "\n",
        "\n",
        "*   split() 방법을 통해서 string을 리스트로 split할 수 있습니다.\n",
        "    * 각 단어는 리스트로 이루워져 있습니다\n",
        "*   Default으로 split()은 whitespace를 통해서 구분하지만 바꿀 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ShMrAYpyRSLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#단어 토큰화\n"
      ],
      "metadata": {
        "id": "88ckl_35Rd32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUqJ9nUMEjO9",
        "outputId": "1266f7b9-041c-433e-a03a-d77ca8b5c95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "# Split text by whitespace\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 보는 것처럼 'language'와 'modeling'은 마지막에 점이 포함되어 있습니다. 파이썬의 split방법은 점을 토큰화하는데 있어서 seperator로 인지하지 않습니다."
      ],
      "metadata": {
        "id": "juq_-TcsRn_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 토큰화"
      ],
      "metadata": {
        "id": "m12FnPxERrqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets split the given text by full stop (.)\n",
        "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
        "text.split(\". \") # Note the space after the full stop makes sure that we dont get empty element at the end of list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_edacVVRp01",
        "outputId": "b2713a97-3f17-4dc3-d21f-6966c35cfe0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
              " 'But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막에 split() 방법은 마지막에 !로 끝나기 때문에 구분하지 못하였습니다. 그래서 다중 구분을 하려면 서로 다른 seperator를 사용하는 split()을 사용하면 되지만 더 좋은 방법이 있습니다.\n"
      ],
      "metadata": {
        "id": "mD9EY2v0Ry5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "정규 표현식을 이용한 토큰화\n"
      ],
      "metadata": {
        "id": "AqxMXPKiR0Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   정규 표현식은 검색할 수 있는 패터으로 이루어진 문자 시퀀스입니다\n",
        "*   RegEx를 사용하면 문자열의 문자 조합을 응용할 수 있고 단어/문장 토큰화를 수행할 수 있습니다.\n",
        "*   RegeEx 관련 작업에 파이썬의 re 라이브러리를 사용할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "TXbTkihIR6b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화"
      ],
      "metadata": {
        "id": "qsB4_SK1S-bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "tokens = re.findall(\"[\\w]+\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN1idVagR4rD",
        "outputId": "311d1290-52ca-43f9-a9df-0e2fe47dde8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "RegEx 패턴을 통해서 단어들의 리스트로 생성할 수 있습니다. RegEx 패턴 각 character의 구체적인 설명은 아래에 있습니다.\n",
        "\n",
        "    [] :    character의 집합\n",
        "    \\w :    문자열에 단어 문자(a에서 Z까지의 문자, 0에서 9까지의 숫자 및 밑줄 _ 문자)가 포함된 일치를 반환합니다\n",
        "    +  :    1 이상 존재할 때 사용합니다\n",
        "\n",
        "그래서 우리의 RegEx 패턴은 코드가 다른 문자가 발생할 때까지 모든 영숫자 문자를 찾아야 한다는 것을 의미합니다."
      ],
      "metadata": {
        "id": "0ixChHs6TB08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화"
      ],
      "metadata": {
        "id": "i6QiKssoTZ6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
        "tokens_sent = re.compile('[.!?] ').split(text) # Using compile method to combine RegEx patterns\n",
        "tokens_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDHUbwpSTark",
        "outputId": "ca062b39-5fe3-46e6-87d5-2a8d534e046a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
              " 'But one drawback with split() method, that we can only use one separator at a time',\n",
              " 'So sentence tonenization wont be foolproof with split() method.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 결과에서 볼 수 있듯이 여러가지의 seperator들을 통해서 문장을 나눌 수 있습니다."
      ],
      "metadata": {
        "id": "oz8DdMu9Tfaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK을 이용한 토큰화\n",
        "\n",
        "\n",
        "*   Naturak Language Toolkit(NLTK)는 자연어 처리를 위해 만들어진 파이썬 라이브러리입니다.\n",
        "*   NLTK는 word_tokenize()를 통해 단어 토큰화를 할 수 있고 sent_tokenize()를 통해 문장 토큰화를 할 수 있습니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lM3zRIA-TiQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화"
      ],
      "metadata": {
        "id": "0lMoa6-GUNb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK 라이브러리 설치\n",
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbTK7zYHUPIB",
        "outputId": "a73d273d-7b6c-43a1-b875-91ab323af3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWv7k-o_URDm",
        "outputId": "d0ed0dc5-31f2-4525-ac95-cd70969a528f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK 단어 토큰화를 진행했을때 점은 토큰이라고 인식되었습니다."
      ],
      "metadata": {
        "id": "V82nBjQVUVmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화"
      ],
      "metadata": {
        "id": "ueg2MLWeUX5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "872hvnrJUU-k",
        "outputId": "f38eb15b-2692-45cf-d47a-e9a2c54c1066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
              " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
              " 'So sentence tonenization wont be foolproof with split() method.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##불용어 제거(Stopwords)\n",
        "\n",
        "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.\n",
        "\n",
        "물론 불용어는 개발자가 직접 정의할 수도 있습니다. 이번에는 영어 문장에서 NLTK가 정의한 영어 불용어를 제거하는 실습을 하겠습니다.\n",
        "\n",
        "NLTK 라이브러리을 사용하기 위해서는 NLTK Data가 필요합니다. 만약, 데이터가 없다는 에러가 발생 시에는 nltk.download(필요한 데이터)라는 커맨드를 통해 다운로드 할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "SV1A6ChhUuST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-6A8rRWYLsk",
        "outputId": "7b2d663c-4645-40db-e3e7-6708dacff359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. NLTK에서 불용어 확인하기\n",
        "\n"
      ],
      "metadata": {
        "id": "-ElgSWFgYN77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqOoQj9KYV-z",
        "outputId": "effa490d-5025-4cd8-f847-25e03224b1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopwords.words(\"english\")는 NLTK가 정의한 영어 불용어 리스트를 리턴합니다. 출력 결과가 100개 이상이기 때문에 여기서는 간단히 10개만 확인해보았습니다. 'i', 'me', 'my'와 같은 단어들을 NLTK에서 불용어로 정의하고 있음을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "vUi6aXrnYYbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. NLTK를 통해서 불용어 제거하기\n",
        "\n"
      ],
      "metadata": {
        "id": "-jMNNXM2Ya_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens:\n",
        "    if word not in stop_words:\n",
        "        result.append(word)\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens)\n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVcVW6_zYfKq",
        "outputId": "4e296137-f674-4365-876b-14d28971da40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드는 \"Family is not an important thing. It's everything.\"라는 임의의 문장을 정의하고, NLTK의 word_tokenize를 통해서 단어 토큰화를 수행합니다. 그리고 단어 토큰화 결과로부터 NLTK가 정의하고 있는 불용어를 제외한 결과를 출력하고 있습니다. 결과적으로 'is', 'not', 'an'과 같은 단어들이 문장에서 제거되었음을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "sHhRKZfeYiN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CountVectorizer 클래스로 BoW 만들기\n",
        "사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원합니다. 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있습니다. CountVectorizer로 간단하고 빠르게 BoW를 만드는 실습을 진행해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "xLkrldKnatyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['you know I want your love. because I love you.']\n",
        "vector = CountVectorizer()\n",
        "\n",
        "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print('bag of words vector :', vector.fit_transform(corpus).toarray())\n",
        "\n",
        "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
        "print('vocabulary :',vector.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Do8A5pbWLx",
        "outputId": "0e8e3fae-78c7-4c4b-e788-ba712e93eb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 2 1 2 1]]\n",
            "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제 문장에서 you와 love는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있습니다. 또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 합니다.\n",
        "\n",
        "주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점입니다. 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미합니다."
      ],
      "metadata": {
        "id": "OR7O69GDbkGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "자연어 처리에 필수적인 작업인 단어 토큰화부터 CountVertorizer까지 기본적인 자연어에 대하여 실습을 통해 배울 수 있었습니다.\n",
        "\n",
        "예를 들어 영어 같은 경우에는 문장을 단어 토큰화나 문장 토큰화로 나눠서 리스트 형태로 넣어 줄 수 있습니다. 그리고 의미 없는 불용어를 제거해 주거나 직접 딕셔너리를 만들어서 제거해 줄 수 있습니다. 그리고 CountVertorizer를 적용하여 빈도수에 의해 단어들을 딕셔너리 형태로 만들어 줄 수 있습니다. 최종적으로 이러한 딕셔너리를 사용하여 감정분석, 문장 요약 등에서 응용 가능할 것 같습니다."
      ],
      "metadata": {
        "id": "MEeKAyTjIQH_"
      }
    }
  ]
}