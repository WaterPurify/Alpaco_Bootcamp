{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk/3Er771i0+Uj2sZse0kF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonwanggyu/Alpaco_Project/blob/main/%EA%B0%9D%EC%B2%B4%EC%9D%B8%EC%8B%9D_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8(06.03~06.20)/%EB%B2%BC_%EC%83%9D%EC%9C%A1%EC%9D%B4%EC%83%81_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 벼 생육이상 Inference"
      ],
      "metadata": {
        "id": "ta-UuAIxVKL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 로드"
      ],
      "metadata": {
        "id": "9R43QYvSVQh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "collapsed": true,
        "id": "vB5SCHqHVJpP",
        "outputId": "dfe1e315-737b-440d-a97c-960b394d2c66"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\xe3'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3fad51b03868>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m  \u001b[0;31m# fine-tuning 데이터셋의 클래스 수에 맞게 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 768은 B3의 내부 차원 수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/best_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \"functionality.\")\n\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xe3'."
          ]
        }
      ],
      "source": [
        "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"nvidia/segformer-b3-finetuned-cityscapes-1024-1024\"\n",
        "# we are going to do whole inference, so no resizing of the image\n",
        "processor = SegformerImageProcessor(do_resize=False)\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
        "\n",
        "# 모델의 헤드 클래스 수를 변경\n",
        "model.config.num_labels = 6  # fine-tuning 데이터셋의 클래스 수에 맞게 변경\n",
        "model.decode_head.classifier = torch.nn.Conv2d(768, model.config.num_labels, kernel_size=1)  # 768은 B3의 내부 차원 수\n",
        "model.load_state_dict(torch.load(\"/content/best_model.pth\", map_location=device))\n",
        "\n",
        "\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "a9l5FXxDVUg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 로드"
      ],
      "metadata": {
        "id": "eo7QvfOGVXT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# load image + ground truth map\n",
        "image_path = \"C:/Users/USER/Desktop/512x288/resized_valid_image/NIA_AgricultureAD_paddy_RGB_bottom_Gyeongsangnamdo_2110151007_day_sunny_001894.jpg\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "arXtl3K0VWXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 전처리"
      ],
      "metadata": {
        "id": "bC-_vqFrVczP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "\n",
        "# 전처리 파이프라인 설정\n",
        "def preprocess_image(image_path, transforms):\n",
        "    # 이미지 파일을 열고 RGB로 변환\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # 전달된 전처리 변환을 이미지에 적용\n",
        "    if transforms:\n",
        "        image = transforms(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# 이미지 경로\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# 전처리 변환 (훈련과 동일)\n",
        "inference_transforms = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 이미지 전처리\n",
        "processed_image = preprocess_image(image_path, inference_transforms)\n",
        "\n",
        "# 차원 조정 (batch_size, C, H, W)\n",
        "processed_image = processed_image.unsqueeze(0)  # 이는 모델에 입력하기 위해 필요한 배치 차원을 추가합니다.\n",
        "\n",
        "processed_image.shape\n",
        "processed_image= processed_image.to(device)"
      ],
      "metadata": {
        "id": "b4LZftckVbPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(processed_image)\n",
        "  logits = outputs.logits"
      ],
      "metadata": {
        "id": "o5mWb98xVfwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "upsampled_logits = F.interpolate(logits, size=(288 ,512), mode=\"bilinear\", align_corners=False)\n",
        "# 소프트맥스를 적용하여 픽셀별 클래스 확률을 계산\n",
        "probabilities = F.softmax(upsampled_logits, dim=1)\n",
        "\n",
        "# 각 픽셀에서 최대 확률과 해당 인덱스를 계산\n",
        "max_probs, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "# 임계값 설정 (예: 0.5)\n",
        "# threshold = 0.3\n",
        "\n",
        "# 임계값 미만인 픽셀을 배경으로 설정\n",
        "# predicted_classes[max_probs < threshold] = 0\n",
        "\n",
        "\n",
        "# Tensor를 Numpy 배열로 변환\n",
        "mask_np = predicted_classes.cpu().numpy()"
      ],
      "metadata": {
        "id": "U9ZUg6_SVkM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시각화"
      ],
      "metadata": {
        "id": "ZbVVsnOk5F2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_segmentation(image, mask, alpha=1):\n",
        "    \"\"\"\n",
        "    이미지와 세그멘테이션 마스크를 시각화합니다.\n",
        "    :param image: PIL 이미지 또는 NumPy 배열\n",
        "    :param mask: 세그멘테이션 마스크 (numpy 배열)\n",
        "    :param alpha: 마스크의 투명도\n",
        "    \"\"\"\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        image = np.array(image)\n",
        "\n",
        "    mask = mask.squeeze()\n",
        "    # 12개 클래스에 대한 색상 매핑 정의\n",
        "    colors = np.array([\n",
        "        [0, 0, 0],        # 배경\n",
        "        [128, 64, 128],   # 클래스 1 - common_road\n",
        "        [244, 35, 232],   # 클래스 2 - common_tree\n",
        "        [70, 70, 70],     # 클래스 3 - field_corps\n",
        "        [102, 102, 156],  # 클래스 4 - field_furrow\n",
        "        [190, 153, 153],  # 클래스 5 - field_levee\n",
        "        [153, 153, 153],  # 클래스 6 - orchard_road\n",
        "        [250, 170, 30],   # 클래스 7 - orchard_tree\n",
        "        [220, 220, 0],    # 클래스 8 - paddy_after_driving\n",
        "        [107, 142, 35],   # 클래스 9 - paddy_before_driving\n",
        "        [152, 251, 152],  # 클래스 10 - paddy_edge\n",
        "        [70, 130, 180],   # 클래스 11 - paddy_rice\n",
        "        [220, 20, 60]     # 클래스 12 - paddy_water\n",
        "    ], dtype=np.uint8)\n",
        "\n",
        "    colored_mask = colors[mask]\n",
        "    combined = image.astype(np.float32) * (1 - alpha) + colored_mask.astype(np.float32) * alpha\n",
        "    combined = combined.clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(combined)\n",
        "    plt.title('Segmentation Overlay')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 이미지와 예측된 마스크 데이터를 이 함수에 전달하여 시각화\n",
        "# image는 PIL 이미지, predicted_classes는 위 코드에서 계산된 예측 클래스 numpy 배열\n"
      ],
      "metadata": {
        "id": "0IJJhzbk5FFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ground Truth"
      ],
      "metadata": {
        "id": "rF7LR7OiKRlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from skimage.draw import polygon2mask\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "id2label = {\n",
        "            0: 'background',\n",
        "            1: 'common_road',\n",
        "            2: 'common_tree',\n",
        "            3: 'field_corps',\n",
        "            4: 'field_furrow',\n",
        "            5: 'field_levee',\n",
        "            6: 'orchard_road',\n",
        "            7: 'orchard_tree',\n",
        "            8: 'paddy_after_driving',\n",
        "            9: 'paddy_before_driving',\n",
        "            10: 'paddy_edge',\n",
        "            11: 'paddy_rice',\n",
        "            12: 'paddy_water'\n",
        "        }\n",
        "\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "mask = np.full((image.height, image.width), 0, dtype=np.int32)  # 초기값을 0으로 설정\n",
        "image_np=np.array(image)\n",
        "\n",
        "base_path= \"C:/Users/USER/Desktop/512x288/resized_valid_annotations\"\n",
        "annotation_path= os.path.join(base_path, image_path.split('/')[-1].split('.')[0]+'.json')\n",
        "\n",
        "with open(annotation_path, 'r') as f:\n",
        "    img_info= json.load(f)\n",
        "\n",
        "for obj in img_info['objects']:\n",
        "    class_id = label2id.get(obj['label'])\n",
        "    for pos in obj['position']:\n",
        "        coords = [(y, x) for x, y in zip(pos[::2], pos[1::2])] #pos[::2]는 짝수 인덱스(모든 x 좌표), pos[1::2]는 홀수 인덱스(모든 y 좌표)\n",
        "        # print(f\"Coords for {obj['label']} with class ID {class_id}: {coords}\")  # 디버깅용 좌표 출력\n",
        "\n",
        "            # 좌표가 이미지 경계를 벗어나는지 확인\n",
        "        out_of_bounds_coords = [(x, y) for y, x in coords if x < 0 or x >= image.width or y < 0 or y >= image.height]\n",
        "        if out_of_bounds_coords:\n",
        "            #print(f\"Warning: Some coordinates for {obj['label']} are out of image bounds: {out_of_bounds_coords}\")\n",
        "            coords = [(max(0, min(image.height - 0.1, y)), max(0, min(image.width - 0.1, x))) for y, x in coords] # 이미지 벗어나는 좌표 클리핑\n",
        "\n",
        "\n",
        "\n",
        "        poly_mask = polygon2mask((image.height, image.width), coords)\n",
        "\n",
        "        # 디버깅용으로 poly_mask가 True인 위치 출력\n",
        "        # true_indices = np.where(poly_mask)\n",
        "        # true_positions = list(zip(true_indices[0], true_indices[1]))\n",
        "        # print(f\"True positions in poly_mask: {true_positions[:10]}\")  # 처음 10개의 위치만 출력\n",
        "\n",
        "        mask[poly_mask] = class_id\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_np)\n",
        "plt.title(\"Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title(\"Mask\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DP3XIcuaKQOB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}