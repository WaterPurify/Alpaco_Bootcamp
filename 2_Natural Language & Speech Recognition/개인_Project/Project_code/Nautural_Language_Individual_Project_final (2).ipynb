{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 네이버 영화 리뷰 데이터 (nsmc) 감성 분석 - LSTM,RNN,GRU\n",
        "\n",
        "* Dataset   \n",
        "    *   네이버 영화 리뷰 데이터 (naver sentiment movie corpus)\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/e9t/nsmc\">네이버 영화 리뷰 데이터</a>\n",
        "  </td>\n",
        "</table>\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "iTukqR62xKgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 불러오기\n",
        "\n",
        "\n",
        "\n",
        "*   모델 적용할 경우 필요한 라이브러리를 불러옵니다\n",
        "\n",
        "    *   시각화: matplotlib,seaborn\n",
        "    *   모델링: pandas,numpy,tensorflow\n",
        "\n",
        "* 한국어 형태소 분리를 위해 Okt 적용하였습니다.\n",
        "* 단어 사전을 만들기 위해서 파이썬 Counter 라이브러리 사용하였습니다.\n",
        "* 분석을 위해 sklearn 라이브러리 사용하였습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uxj3jsah1GKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.16.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfeHApCIHq1R",
        "outputId": "4457ff48-fe89-4695-eb5f-827d8dd0424f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.24.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekQPPgK94tss",
        "outputId": "668635a9-a177-4ec1-bcdb-aabf3f540378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.24.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델링\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# sklearn과 Counter 라이브러리\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 한국어 형태소 Okt\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "ZWxIAZROzLqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "* train과 test 데이터셋\n",
        "\n"
      ],
      "metadata": {
        "id": "86cyFW8O3opw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZd86RQgxFjw"
      },
      "outputs": [],
      "source": [
        "train = pd.read_table(\"/content/ratings_train.txt\")\n",
        "test = pd.read_table(\"/content/ratings_test.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()\n",
        "test.head()"
      ],
      "metadata": {
        "id": "tGtM-KuI42sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train shape => {train.shape} \\ntest shape => {test.shape}\")\n",
        "train.columns"
      ],
      "metadata": {
        "id": "GSZ3NfGy4_o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시각화\n",
        "\n",
        "* Seaborn의 countplot과 matplotlib의 pie chart를 적용하여 train과 test 개수를 확인 가능합니다"
      ],
      "metadata": {
        "id": "LgVhAj_U5WAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style = \"darkgrid\")\n",
        "ax = sns.countplot(x = 'label',data = train)"
      ],
      "metadata": {
        "id": "1Qphy5cg5U_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style = \"darkgrid\")\n",
        "ax = sns.countplot(x = 'label',data = test)"
      ],
      "metadata": {
        "id": "wPdoW11C6GvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels,frequencies = np.unique(train.label.values, return_counts = True)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(frequencies,labels= labels,autopct ='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "27i6JECh7fDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels,frequencies = np.unique(test.label.values, return_counts = True)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(frequencies,labels= labels,autopct ='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7T6SO1YE8gUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 라벨 값이 균등하게 train과 test에서 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "2lEHpn6V8lDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 값 확인\n",
        "\n"
      ],
      "metadata": {
        "id": "DU5Yoq_B99cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 데이터의 결측 값\n",
        "train.isnull().sum()\n"
      ],
      "metadata": {
        "id": "aIEfM2cY-CxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터의 결측 값\n",
        "test.isnull().sum()"
      ],
      "metadata": {
        "id": "dVeV4IhA-NZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "\n",
        "\n",
        "*   데이터 중복 제거 (drop_duplicate)\n",
        "*   결측 값 제거 (dropna)\n",
        "*   한국어만 추출 (정규표현식 사용)\n",
        "*   한국어 토큰화 (Okt)\n",
        "*   불용어 제거 (조사,구두점,접미사 종류)\n",
        "*   데이터에 정수 인코딩 (text_to_sequence)\n",
        "*   문장 길이 분포와 적절한 최대 문장 길이 지정\n",
        "*   최대 문자 길이에 따른 패딩 추가 (pad_sequences)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gI06HiCD-Ynh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복값 제거\n",
        "train.drop_duplicates(subset =['document'],inplace=True)\n",
        "test.drop_duplicates(subset = ['document'],inplace=True)\n",
        "print(train)\n",
        "print(test)"
      ],
      "metadata": {
        "id": "DP8O99AJCTI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #결측값 제거\n",
        "    train = train.dropna()\n",
        "    test = test.dropna()\n",
        "    print(train)\n",
        "    print(test)"
      ],
      "metadata": {
        "id": "WWaDQLPHCYT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어만 추출\n",
        "train['document'] = train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train['document'].replace('', np.nan, inplace=True)\n",
        "\n",
        "test['document'] = test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "test['document'].replace('', np.nan, inplace=True)\n"
      ],
      "metadata": {
        "id": "5FjhNq98C_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 토큰화\n",
        "okt = Okt()\n",
        "\n",
        "# 불용어 제거\n",
        "stopwords = ['요', '도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
        "\n",
        "# 토큰화 적용\n",
        "train['tokenized'] = train['document'].apply(okt.morphs)\n",
        "train['tokenized'] = train['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
      ],
      "metadata": {
        "id": "FLB6KQodD067"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['tokenized'] = test['document'].apply(okt.morphs)\n",
        "test['tokenized'] = test['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
      ],
      "metadata": {
        "id": "DvNFA2E0jZfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['tokenized']\n",
        "y_train = train['label']\n",
        "X_test= test['tokenized']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "cQLWtkc4D8LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train)\n",
        "# print(y_train)\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "\n",
        "\n",
        "# X_train = pd.DataFrame(X_train)\n",
        "# X_train.to_csv('train_tokenized.csv')\n",
        "# y_train.to_csv('train_label.csv')\n",
        "# X_test.to_csv('test_tokenized.csv',encoding='utf-8-sig')\n",
        "# y_test.to_csv('test_label.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mKaIiiVRLQ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = 'train_tokenized.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "data"
      ],
      "metadata": {
        "id": "EfEy1m06S7ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 데이터를 빈도수 기준으로 단어 집합을 생성\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# 맵핑하는 함수 text_to_sequences 적용\n",
        "X_train_sequence= tokenizer.texts_to_sequences(X_train)"
      ],
      "metadata": {
        "id": "5XVYoqRyHKZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "rare_cnt = 0\n",
        "total_freq = 0\n",
        "rare_freq = 0\n",
        "\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1ijkhfP2Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 일반적으로 0번 토큰(padding)과 1번 토큰 (OOV)를 피하기 위해 2부터 시작\n",
        "vocab_size = total_cnt- rare_cnt +2\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "metadata": {
        "id": "hDG0k6LkIP5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val= tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "4E8ce7uvxVps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_WEqz0VJefL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 80 #Padding 80으로\n",
        "X_train = pad_sequences(X_train_sequence, maxlen = max_len)\n",
        "X_val = pad_sequences(X_val, maxlen = max_len)\n",
        "\n",
        "sum =0\n",
        "for i in X_train:\n",
        "    sum+=1\n",
        "print(sum)"
      ],
      "metadata": {
        "id": "IL5QP_zMi_zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "ccKPBbBvAM70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU 모델 이용한 감성 분석\n",
        "\n"
      ],
      "metadata": {
        "id": "QzmhDdAD3lBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#필용한 라이브러리 불러옵니다\n",
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "Q8dttA6O3tta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU 모델을 적용\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "7BEqZ8FW32AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "myAKZXxS3_-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "metadata": {
        "id": "25MHCYoVQDNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_GRU = model.fit(X_train, y_train, epochs=15, batch_size=60, validation_split=0.2)"
      ],
      "metadata": {
        "id": "aBMuS7um4OHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM 모델을 아용한 감성 분석"
      ],
      "metadata": {
        "id": "EyOSvnGg9WS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델을 적용\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional"
      ],
      "metadata": {
        "id": "sroe1mVX9frr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, 100))\n",
        "model_lstm.add(LSTM(100)) #Bidirectional drop\n",
        "model_lstm.add(Dense(1, activation='relu')) #activation = sigmoid -> relu"
      ],
      "metadata": {
        "id": "Nx8c0PIx9cec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "WZBUnrxN9oiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "udwLUaOX-PTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_LSTM = model_lstm.fit(X_train, y_train, epochs=15, batch_size=60, validation_split=0.2)"
      ],
      "metadata": {
        "id": "qHeG9GBl9tmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 데이터셋에 모델 평가\n",
        "\n",
        "\n",
        "\n",
        "*   loss 값과 accuracy값이 출력\n",
        "\n"
      ],
      "metadata": {
        "id": "kgEjBT2VKozx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM로 학습된 모델로 평가\n",
        "predict_LSTM = model_lstm.evaluate(X_test,y_test,verbose=1)\n",
        "print(predict_LSTM)"
      ],
      "metadata": {
        "id": "s_hwGsWFNCZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict_LSTM = history_LSTM.history\n",
        "print(history_dict_LSTM.keys())"
      ],
      "metadata": {
        "id": "MWVJgzk8MWyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict_LSTM['accuracy']\n",
        "val_acc = history_dict_LSTM['val_accuracy']\n",
        "loss = history_dict_LSTM['loss']\n",
        "val_loss =history_dict_LSTM['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.lengend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YAF0z1nSLqWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRU로 학습된 모델로 평가\n",
        "predict_GRU = model.evaluate(X_test,y_test,verbose=1)\n",
        "print(predict_GRU)"
      ],
      "metadata": {
        "id": "lbZnxswtLeJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict_GRU = history_GRU.history\n",
        "print(history_dict_GRU.keys())"
      ],
      "metadata": {
        "id": "ws-g-apKM6dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict_GRU['accuracy']\n",
        "val_acc = history_dict_GRU['val_accuracy']\n",
        "loss = history_dict_GRU['loss']\n",
        "val_loss =history_dict_GRU['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs,loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss,'b',label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.lengend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_uhs5R6M2n6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}