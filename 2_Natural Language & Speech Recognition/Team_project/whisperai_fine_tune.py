# -*- coding: utf-8 -*-
"""WhisperAI_fine_tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17voT0F3PXidtKAdgO8b4TM1xhbhMPeBD

# ü§ó TransformerÎ•º Ï†ÅÏö©Ìïú Îã§Íµ≠Ïñ¥ ÏûêÎèô ÏùåÏÑ± (ASR)ÏùÑ WhisperÏóê fine-tuning

*   Step-by-step guide on how to fine-tune Whisper for any multilingual ASR dataset using Hugging Face ü§ó Transformers.
*   For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post.[blog post](https://huggingface.co/blog/fine-tune-whisper).

## Introduction

*   Whisper
    * A pre-trained model for automatic speech recognition (ASR) published in September 2022 by the authors Alec Radford et al. from OpenAI.
    *  Pre-trained on a vast quantity of labelled audio-transcription data, 680,000 hours to be precise
    * Demonstrate a strong ability to generalise to many datasets and domains

<figure>
<img src="https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg" alt="Trulli" style="width:100%">
<figcaption align = "center"><b>Figure 1:</b> Whisper model. The architecture
follows the standard Transformer-based encoder-decoder model. A
log-Mel spectrogram is input to the encoder. The last encoder
hidden states are input to the decoder via cross-attention mechanisms. The
decoder autoregressively predicts text tokens, jointly conditional on the
encoder hidden states and previously predicted tokens. Figure source:
<a href="https://openai.com/blog/whisper/">OpenAI Whisper Blog</a>.</figcaption>
</figure>

* Five configurations of varying model sizes.
    1. The smallest four trained on either English-only or multilingual data.
    2. The largest checkpoints  multilingual only.
    
* All 11 of the pre-trained checkpoints are available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper).

* The checkpoints are summarised in the following table with links to the models on the Hub:

| Size     | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                        |
|----------|--------|-------|-------|------------|------------------------------------------------------|-----------------------------------------------------|
| tiny     | 4      | 384   | 6     | 39 M       | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny.)    |
| base     | 6      | 512   | 8     | 74 M       | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |
| small    | 12     | 768   | 12    | 244 M      | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |
| medium   | 24     | 1024  | 16    | 769 M      | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |
| large    | 32     | 1280  | 20    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large)    |
| large-v2 | 32     | 1280  | 20    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |
| large-v3 | 32     | 1280  | 20    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v3) |


* Fine-tune the multilingual version of the
[`"small"`](https://huggingface.co/openai/whisper-small) checkpoint with 244M params (~= 1GB).
* Train and evaluate our system on a low-resource language
taken from the [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)
dataset.
* 8 hour fine-tuning of Whisper AI can acheive strong performance in our Korean language for old peaple.

## Prepare Environment

1. GPU setting
    *  click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _CPU_ to one of the available GPUs, e.g. _T4_ (or better if you have one available). Next, click `Connect T4` in the top right-hand corner of your screen (or `Connect {V100, A100}` if you selected a different GPU).

We can verify that we've been assigned a GPU and view its specifications:
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""* Employ poplular Pytohn packages

    1. Use `datasets[audio]` to download and prepare our training data
    2. `Transformers` and `accelerate` to load and train our Whisper model
    3. `soundfile` package to pre-process audio files
    4. `evaluate` and `jiwer` to assess the performance of our model
    5. `tensorboard` to log our metrics
    6.  `gradio` to build a
flashy demo of our fine-tuned model.
"""

!pip install --upgrade --quiet pip
!pip install --upgrade --quiet datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio

#spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.
!pip install typer==0.9.1

#tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.

!pip install tensorboard==2.15.0

#weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.
!pip install weasel==0.3.4

"""* Upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)
whilst training.

* The Hub provides:
    - Integrated version control: you can be sure that no model checkpoint is lost during training.
    - Tensorboard logs: track important metrics over the course of training.
    - Model cards: document what a model does and its intended use cases.
    - Community: an easy way to share and collaborate with the community!

- Linking the notebook to the Hub is straightforward  

* Entering your
Hub authentication token when prompted. Find your Hub authentication token [here](https://huggingface.co/settings/tokens):
"""

from huggingface_hub import notebook_login

notebook_login()

"""# Load Dataset

* Using ü§ó Datasets, download and prepare data

    1. Ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_12_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_12_0).

    2. 5 hours of korean dataset is considered low-resource, so we decided to use dataset from [AI hub](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=123) which has more than 1000 hours with 2000 speakers.


"""

from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset("mozilla-foundation/common_voice_12_0", "ko", split="train+validation", use_auth_token=True)
common_voice["test"] = load_dataset("mozilla-foundation/common_voice_12_0", "ko", split="test", use_auth_token=True)

print(common_voice)

"""* Most ASR datasets only provide input audio samples (`audio`) and the
corresponding transcribed text (`sentence`).

    * Common Voice contains additional
      metadata information, such as `accent` and `locale`, which we can disregard for ASR.

* For only consider the input audio and
transcribed text for fine-tuning, discarding the additional metadata information:
"""

common_voice = common_voice.remove_columns(["accent", "age", "client_id", "down_votes", "gender", "locale", "path", "segment", "up_votes"])

print(common_voice)