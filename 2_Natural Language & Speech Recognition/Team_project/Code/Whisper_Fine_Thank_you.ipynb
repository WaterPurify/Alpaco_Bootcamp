{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-vCWQ3E2vxGY",
        "QmzhV2tDvz6A",
        "osOqju0IGOky"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungsikMoon/FORS/blob/main/Whisper_Fine_Thank_you.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper Fine Tuning"
      ],
      "metadata": {
        "id": "-vCWQ3E2vxGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "XuLGQt6nBVeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "WF6YyHRRBW1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 압축 풀 경로로 이동\n",
        "%cd /content/drive/MyDrive/whisper/[원천]3.스튜디오\n",
        "!pwd"
      ],
      "metadata": {
        "id": "sDkWYwjoBXlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 경로에 압축 풀기 (압축파일 경로 찍어줌)\n",
        "# 압축파일을 먼저 올려놓고 여기서 풀어주는게 훨씬 빠름\n",
        "\n",
        "# 잘 못 누르면 데이터 꼬이니까 꼭 주석처리 해둘 것\n",
        "# !unzip -qq \"/content/drive/MyDrive/whisper/[원천]3.스튜디오.zip\""
      ],
      "metadata": {
        "id": "GpRd1acdBeB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 패키지 설치\n",
        "\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "WlU2nYC9Be8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 오디오 데이터셋 베이스 정보 리스트 만들기"
      ],
      "metadata": {
        "id": "8nzQHzWrBk9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 필수 정보 : 오디오 데이터 파일 경로, 발화 텍스트\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 추가 정보 : json 파일 경로, 오디오 시간 정보\n",
        "- 최종 데이터셋으로 만들기 전 단계\n",
        "- 데이터 검증 및 필수 정보를 담은 리스트 추출 단계\n",
        "- json 파일 열어서 내용 가져오는게 너무 오래 걸려서 로컬에서 만들어와서 편집하기로함\n"
      ],
      "metadata": {
        "id": "S4EVOxtlBnrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 오디오파일 경로 + 텍스트 매칭한 리스트 만들기\n",
        "\n",
        "# 폴더와 파일 리스트를 뽑는데 유용한 라이브러리\n",
        "# os 패키지로도 동일하게 수행 가능하지만 파일 리스트를 편하게 뽑는데 좀 더 특화돼있음\n",
        "# 특히 glob 패키지의 glob 함수는 하위 폴더명이 아닌 하위 폴더의 전체 경로를 리스트로 반환해줘서 아주 편함\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "# 문제는 폴더가 너무 많고 파일명이 복잡하다는 점\n",
        "# 오디오파일과 json 파일 매칭이 제대로 되지 않을 경우 학습의미가 없음\n",
        "    # ==> 데이터 프레임 만들 때 정확히 파일 이름이 일치하는 경우에만 매칭되도록 전처리 필요\n",
        "\n",
        "# 오디오 파일 최상위 경로\n",
        "# 폴더명, 파일명에 대괄호 들어있으면 인식 못함.. 수정할 것(수정 어려우면 os.listdir 사용)\n",
        "audio_file_path = \"/content/drive/MyDrive/whisper/원천_3_스튜디오/*\"\n",
        "\n",
        "# json 파일 최상위 경로\n",
        "json_file_path = \"/content/drive/MyDrive/whisper/라벨_3_스튜디오/*\"\n",
        "\n",
        "\n",
        "# 오디오 파일 구조 : [원천]3.스튜디오(1ea) - 노인남여_xxx폴더s(16ea) - 노인남녀_xxx.wav(다수)\n",
        "# json 파일 구조 : [라벨]3.스튜디오(1ea) - 노인남여_xxx폴더s(16ea) - 노인남녀_xxx.json(다수)\n",
        "\n",
        "# 원천, 라벨의 모든 폴더와 파일 이름이 같고, 같은 순서로 정렬됐는지 무결성 검사를 먼저 진행\n",
        "# 진행하면서 파일 경로 리스트를 먼저 만들어둠\n",
        "\n",
        "# 폴더 경로 리스트 정렬\n",
        "audio_folder_path_list = sorted(glob(audio_file_path))\n",
        "json_folder_path_list = sorted(glob(json_file_path))\n",
        "\n",
        "# 폴더, 파일 갯수 매칭 확인부터. 갯수 다르면 다른 작업 시작할 필요가 없음.\n",
        "# 갯수 하나라도 다르면 데이터 사용 불가.\n",
        "if len(audio_folder_path_list) == len(json_folder_path_list):\n",
        "    for audio_folder_path, json_folder_path in zip(audio_folder_path_list, json_folder_path_list):\n",
        "\n",
        "        # 해당 폴더에 대한 파일 갯수가 같다면 계속 진행\n",
        "        if len(glob(audio_folder_path + \"/*\")) == len(glob(json_folder_path + \"/*\")):\n",
        "            continue\n",
        "\n",
        "        # 다르면 에러 발생\n",
        "        else:\n",
        "            print(f\"파일 갯수 다름 : {audio_folder_path}, {json_folder_path}\")\n",
        "            raise Exception(\"파일 무결성 검증 실패\")\n",
        "\n",
        "    print(\"데이터 갯수 무결성 검증 완료\")\n",
        "\n",
        "else:\n",
        "    print(\"폴더 갯수 다름\")\n",
        "    raise Exception(\"파일 무결성 검증 실패\")\n",
        "\n",
        "# 갯수 검증 끝났으면 다음 단계 (폴더, 파일 이름 확인 후 경로 정보 리스트로 추출)\n",
        "# 오디오, json 파일 경로 리스트\n",
        "audio_file_path_list = []\n",
        "json_file_path_list = []\n",
        "\n",
        "\n",
        "# 첫 번째 루프 : 각 폴더 경로 꺼내옴\n",
        "for audio_folder_path, json_folder_path in zip(audio_folder_path_list, json_folder_path_list):\n",
        "\n",
        "    # 정렬된 파일 리스트 만들기\n",
        "    audio_file_path_list_sorted = sorted(glob(audio_folder_path + \"/*\"))\n",
        "    json_file_path_list_sorted = sorted(glob(json_folder_path + \"/*\"))\n",
        "\n",
        "    # 두 번째 루프 : 각 파일\n",
        "    for audio_file_path, json_file_path in zip(audio_file_path_list_sorted, json_file_path_list_sorted):\n",
        "\n",
        "        # 확장자 제외 파일 이름이 같은지 확인\n",
        "        if audio_file_path.split(\"/\")[-1].split(\".\")[0] == json_file_path.split(\"/\")[-1].split(\".\")[0]:\n",
        "\n",
        "            # 이름 같으면 해당 파일 경로를 각 리스트에 넣어줌\n",
        "            audio_file_path_list.append(audio_file_path)\n",
        "            json_file_path_list.append(json_file_path)\n",
        "\n",
        "        # 이름이 다르다면 데이터 매칭이 꼬여있다는 의미이므로 에러 발생 후 데이터 확인 필요\n",
        "        else:\n",
        "            print(f\"파일 이름 불일치 : {audio_file_path}, {json_file_path}\")\n",
        "            raise Exception(\"파일 무결성 검증 실패\")\n",
        "\n",
        "print(f\"폴더 및 파일 이름 매칭 검증 및 리스트 생성 완료, 갯수 : {len(audio_file_path_list)} : {len(json_file_path_list)}\")\n",
        "\n",
        "\n",
        "# 완성된 리스트에서 json 파일 안에 text 정보만 빼서 새로운 리스트 생성\n",
        "# 녹음시간 통계를 보고 싶어서 시간정보도 추출\n",
        "text_list = []\n",
        "time_list = []\n",
        "\n",
        "for json_file_path in json_file_path_list:\n",
        "    with open (json_file_path, \"r\") as f:\n",
        "        dict = json.load(f)\n",
        "        text_list.append(dict[\"발화정보\"][\"stt\"])\n",
        "        time_list.append(dict[\"발화정보\"][\"recrdTime\"])\n",
        "\n",
        "# 갯수 확인\n",
        "print(len(text_list), len(time_list))"
      ],
      "metadata": {
        "id": "f3ro0dOBBo1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - 최종 데이터 프레임 생성"
      ],
      "metadata": {
        "id": "D4JcZF2QBqzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 코랩 json 읽기 느려서 로컬에서 작업한 파일 가져와서 편집\n",
        "- 오디오 경로, json 경로, 발화텍스트, 녹음시간"
      ],
      "metadata": {
        "id": "pHLgqbcHBr6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 로컬에서 작업한 내용 코랩용으로 만들어서 다시 저장해주기\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/whisper/audio_path.csv\", encoding = \"utf-8\")\n",
        "\n",
        "# 루트 위치 바꿔주기\n",
        "df[\"audio_path\"] = df[\"audio_path\"].str.replace(\"E:\\\\02.공부\\\\02.코딩\\\\01.Python\\\\01.Alphaco\\\\02.코드\\\\03.프로젝트\\\\02.자연어처리(음성인식)\\\\02.위스퍼파인튜닝\\\\01.data\\\\01.노인발화자유음성(스튜디오3)\\\\\", \"/content/drive/MyDrive/whisper/\")\n",
        "# 로컬에서는 \\\\, 코랩에서는 /로 구분. 바꿔주기\n",
        "df[\"audio_path\"] = df[\"audio_path\"].str.replace(\"\\\\\", \"/\")\n",
        "\n",
        "# 새로 저장\n",
        "df.to_csv(\"/content/drive/MyDrive/whisper/audio_path_colab.csv\", index = False)"
      ],
      "metadata": {
        "id": "ZaI5tZorBtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - 데이터 프레임 불러오기 및 녹음파일 테스트"
      ],
      "metadata": {
        "id": "_Gb8-djhBukK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/whisper/audio_path_colab.csv\", encoding = \"utf-8\")\n",
        "\n",
        "# 경로 잘 되는지 테스트\n",
        "from IPython.display import Audio\n",
        "Audio(df.iloc[0, 0], rate=16000)"
      ],
      "metadata": {
        "id": "DcxdoRGmBv9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DatasetDict 데이터셋 만들기\n",
        "\n"
      ],
      "metadata": {
        "id": "aXeIEmiiBxI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위에서 만들어진 \"오디오파일 경로\", \"발화 텍스트\" 리스트를 이용해 최종 자료형 생성\n",
        "\n",
        "- DatasetDict 자료형이 위스퍼 최종 학습을 위한 자료형으로 사용됨\n",
        "- 위스퍼 인풋 음성 파일의 Sample rate는 16khz로 고정\n",
        "- Train, Vaildation, Test 세 개로 나눠진 최종 DatasetDict 자료형 생성\n"
      ],
      "metadata": {
        "id": "I8uyU1K9ByCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제부터 시작..\n",
        "\n",
        "# 뽑은 파일 경로 데이터를 위스퍼가 요구하는 데이터셋 형태로 변환\n",
        "# 최종적으로 DatasetDict 객체로 만들어줘야함\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# 오디오 파일의 sampling rate를 조절해줌\n",
        "# 위스퍼에서는 무조건 16khz(16,000hz)로 맞춰줘야함.\n",
        "from datasets import Audio\n",
        "\n",
        "# 오디오파일 경로와 텍스트를 딕셔너리 형태로 넣어줌\n",
        "# Dataset에서 제공하는 cast_column 함수에서 audio의 value값에 Audio함수를 적용해 sampling_rate를 조절해줌 (cast_column()은 일종의 map 같은 함수인듯)\n",
        "ds = Dataset.from_dict({\"audio\" : df[\"audio_path\"].tolist(), \"transcripts\" : df[\"text\"].tolist()}).cast_column(\"audio\", Audio(sampling_rate = 16000))\n",
        "\n",
        "# 위의 ds는 전체 데이터\n",
        "# 전체 중에서 validation 데이터와 test 데이터를 분할해줌\n",
        "\n",
        "# 훈련데이터(0.8), validation(0.2) 비율로 나눠줌\n",
        "ds_train_valid = ds.train_test_split(test_size = 0.2)\n",
        "\n",
        "# validation 중에서 절반을 다시 테스트 데이터로 나눠줌\n",
        "# 데이터셋은 딕셔너리 형태로 데이터를 쪼개서 보관하니까 컬럼명으로 찾아주면 됨 (train, test)\n",
        "ds_test = ds_train_valid[\"test\"].train_test_split(test_size = 0.5)\n",
        "\n",
        "# train, validation, test 세 가지로 쪼개진 데이터를 DatasetDict 형태로 한 번에 묶어주면 데이터 가공 끝\n",
        "# 구조가 약간 헷갈릴 수 있음\n",
        "    # 처음 만든 ds_train_valid의 [\"train\"]은 0.8의 훈련 데이터를 의미\n",
        "    # ds_train_valid의 0.2인 [\"test\"]를 다시 반반씩 쪼개서 하나는 validation, 하나는 test로 사용하기로 함\n",
        "        # 따라서 ds_test의 [\"train\"]과 [\"test\"]가 각각 validation, test 자료가 되는 구조 (비율이 반반이니까 순서는 상관없음)\n",
        "datasets = DatasetDict({\"train\" : ds_train_valid[\"train\"], \"valid\" : ds_test[\"train\"], \"test\" : ds_test[\"test\"]})\n",
        "\n",
        "# 허깅페이스에 업로드해서 보관하는 데이터셋은 위의 datasets임.\n",
        "# 구글드라이브에는 이 객체 자체를 저장 못하니까 허깅페이스에서 지원해주는듯\n",
        "# 나중에 체크포인트 등도 지원해주니까 업로드해서 쓰는게 좋을 것 같음\n",
        "\n",
        "datasets[\"train\"]"
      ],
      "metadata": {
        "id": "QPM2GVGOB09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 전처리 완료된 최종 학습용 datasetDict 만들기\n"
      ],
      "metadata": {
        "id": "ZAcIF1bUB2DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 패딩, mel-log로 변환된 input_features 컬럼 / 인덱스로 변환된 labels 컬럼"
      ],
      "metadata": {
        "id": "Diwk9l05B4mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "- 기존 컬럼 (audio, transcripts)는 버리고 전처리 완료된 datasetDict 객체를 생성\n",
        "- 전처리 작업 약 40분 넘게 소요 예정. 객체 저장 필수\n",
        "- 허깅 페이스는 매 번 토큰 입력해줘야함으로 구글 드라이브에 저장\n",
        "- 코랩 메모리 아웃나서 로컬에서 진행..=ㅅ=\n",
        "    - 로컬에서는 10분 걸림..-ㅅ-\n",
        "    - 코랩프로 결제해도 2시간 걸리는데..? 이게 맞는거임..???\n",
        "    - 아무래도 구글 드라이브를 쓰다보니 I/O가 엄청나게 느린듯....\n",
        "    - 전처리는 로컬이 짱인듯.. 근데 이건 또 드라이브에 업로드하는 시간이.....\n",
        "    - GPU 빵빵하게 달린 서버 한 대 쓰는게 최고다.. 코랩프로+도 그닥 소용없을 듯.."
      ],
      "metadata": {
        "id": "FihY9wEzB6Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 오디오파일 전처리(학습전), 후처리(학습후) 해주는 클래스\n",
        "    # WhisperFeatureExtractor 클래스와 WhisperTokenizer 클래스를 상속받아 하나로 쓸 수 있도록 합쳐둔 클래스\n",
        "    # 위 두 개 클래스는 따로 import 하지 않아도 됨\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "# 객체 생성\n",
        "# 파라미터 : 베이스 모델(tiny, small, medium 등), 언어종류, 태스크\n",
        "    # transcribe는 해당 언어로 전사를 뜻함. 바로 영어로 번역하려면 다른 태스크 모드(translate) 사용\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n",
        "\n",
        "\n",
        "# 데이터 전처리 작업을 위한 함수\n",
        "def prepare_dataset(batch):\n",
        "    # 오디오 파일을 16kHz로 로드\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # input audio array로부터 sample_rate값 적용해서 log-Mel spectrogram 변환 및 30초로 패딩작업\n",
        "    # 오디오가 다 짧은데 꼭 30초로 패딩해야하는가에 대한 의문. 줄이는 방법이 안보임. 못줄인다네..-ㅅ-\n",
        "    # input_features[0] : input_features, input_features[1] : attention_mask (위스퍼에서는 30초로 일괄 패딩해버리니까 불필요)\n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate = audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # target text를 label ids로 변환\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"transcripts\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "\n",
        "# 데이터 전처리 함수를 데이터셋 전체에 적용\n",
        "# num_proc : 몇 개의 CPU 코어를 사용할 지 결정 (2개부터 병렬처리)\n",
        "    # 코랩 CPU 코어 확인 : !grep \"cpu cores\" /proc/cpuinfo | tail -1 ==> 1코어밖에 안줌..\n",
        "\n",
        "# remove_columns는 반환할 때 해당 컬럼은 제외한 객체를 반환한다는 의미\n",
        "    # datasets.column_names[\"train\"] : [\"audio\", \"transcripts\"] (오디오, 발화텍스트)\n",
        "    # 즉, \"audio\"에 들어있는 path, array, sampling_rate 정보를 써서 실제 log-Mel로 전처리된 \"input_features\"를 생성\n",
        "    # \"transcripts\"에 들어있는 한국어는 인덱스로 변환시켜 전처리된 \"labels\"를 생성\n",
        "        # 전처리된 데이터를 가진 \"input_features\"와 \"labels\"만 가진 새로운 datasetDict 타입의 객체를 반환 (나머지 컬럼은 제거된)\n",
        "        # 굳이 remove_columns() 옵션을 사용하지 않고 그냥 함수 내에서 작업 후 해당 컬럼을 지워버려도 무방할 듯\n",
        "\n",
        "low_call_voices = datasets.map(prepare_dataset, remove_columns = datasets.column_names[\"train\"], num_proc=1)"
      ],
      "metadata": {
        "id": "9yvSlrgsB5wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - 최종 전처리된 datasetDict 객체 저장하고 불러오기\n"
      ],
      "metadata": {
        "id": "eqonFj27CAxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 이제 데이터 전처리는 끝\n",
        "- 이 객체만 있으면 코랩 말고 다른 곳에서도 학습시킬 수 있음\n",
        "- 이후 프로세스는 TPU 또는 GPU가 필요\n",
        "    - 근데 GPU가 없네.. GPU 내놔라...=ㅅ=\n",
        "\n",
        "- 문제는 최종 전처리된 객체가 3G -> 16G로 용량이 늘어났다는 것..\n",
        "    - 5~10초짜리 음성파일이 대부분인데 30초로 일괄 패딩해버린 탓..\n",
        "    - 패딩 길이 조절하는 방법은 없다고함\n",
        "        - 그냥 30초 내외 음성파일 쓰는게 가장 효율적일 듯"
      ],
      "metadata": {
        "id": "gnrN0wWbCCq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 완료된 datasetDict 객체 디스크(구글 드라이브)에 저장하기 (폴더로 저장됨)\n",
        "    # 드라이브에 15기가 공간 없음. 그냥 content에 저장..\n",
        "# 실제로는 수행안함. 로컬에서 해서 드라이브에 가져다둠. 딥빡..\n",
        "low_call_voices.save_to_disk(\"/content/drive/MyDrive/whisper/low_call_voice\")"
      ],
      "metadata": {
        "id": "XwVl7HPkCDoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 완료된 datasetDict 객체 디스크(구글 드라이브)에서 불러오기\n",
        "from datasets import load_from_disk\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "# 여기서부터 새로 시작.\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n",
        "low_call_voices_prepreocessed = load_from_disk(\"/content/drive/MyDrive/whisper/low_call_voice\")\n",
        "low_call_voices_prepreocessed"
      ],
      "metadata": {
        "id": "2Uus1hGjCEoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 학습 (Fine-Tuning) 진행\n"
      ],
      "metadata": {
        "id": "iMYlpGX8CGUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "- 전처리된 데이터로 학습진행 시작\n",
        "- 여기서부터 GPU/TPU 필수\n",
        "- GPU/TPU 주는 캐글 노트북으로 건너가서 진행\n",
        "    - 아래 코드는 코랩에서는 캐글에서 수행할 것..\n",
        "    - 근데 캐글에 16기가 파일 안올라가서 실패.."
      ],
      "metadata": {
        "id": "77nZ1lliCHIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Data Collator\n"
      ],
      "metadata": {
        "id": "qdA_zdqBCIjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전처리한 데이터를 모델에 입력할 수 있는 PyTorch 텐서 형태로 변환하는 작업\n",
        "- 이 때 패딩이 되지 않은 타겟(labels = 발화데이터)도 같이 패딩처리해줌"
      ],
      "metadata": {
        "id": "tQtq52ACCNg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "# 전처리된 데이터를 텐서 형태로 변환\n",
        "# 제일 이해 안가는 코드..\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features : List[Dict[str, Union[List[int], torch.Tednsor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # featurs는 최종 datasetDict 자료 (3개로 이뤄져있음)\n",
        "            # train : input_features, labels\n",
        "            # valid : input_features, labels\n",
        "            # test : input_features, labels\n",
        "\n",
        "        # 인풋 데이터(mel-log로 변환된 오디오 파일)를 토치 텐서로 변환\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "\n",
        "        # input_feaures를 패딩. 이미 패딩 돼 있는데 왜 또하는지는 잘 모르겠음.\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 라벨 데이터 (정수 인코딩된 발화데이터)를 토치 텐서로 변환\n",
        "        # ex) train이 가진 [\"input_features\"] 컬럼을 딕셔너리 구조를 가진 리스트 안에다가 넣어줌\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # 라벨 데이터 패딩 적용\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 패딩 토큰을 -100으로 치환해 loss 계산 과정에서 무시되도록 함\n",
        "        # 이미 패딩 토큰이라는 것 자체가 무시되도록 돼 있을텐데 왜 하는지를 잘 모르겠음\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라냄\n",
        "        # 이 코드는 뭔지 잘 모르겠음..\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# 데이터 콜레이터 초기화\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "-OTNZREHCOlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Evaluaion Metrics"
      ],
      "metadata": {
        "id": "FiFZnteCCQcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 검증 데이터셋에 사용할 검증 매트릭스 정의\n",
        "- 한국어의 경우 WER 보다 CER이 더 적합"
      ],
      "metadata": {
        "id": "ROlD9EX5CRmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('cer')\n",
        "\n",
        "def compute_metrics(pred):\n",
        "\n",
        "    # 예측값\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # 실제값\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # 패딩된 토큰을 올바르게 무시하기 위해 -100을 다시 pad_token으로 치환\n",
        "    # 밑에 kip_special_tokens=True가 적용될 수 있도록 함\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # 정수 인덱스를 실제 문자열로 디코딩\n",
        "    # metrics 계산 시 special token들을 빼고 계산하도록 설정\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # CER 계산\n",
        "    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ],
      "metadata": {
        "id": "mlGh1nkfCSxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Load a Pre-Trained Checkpoint\n"
      ],
      "metadata": {
        "id": "-J4ORd5lCUTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pre_trained model : 이미 학습된 위스퍼의 기본 모델을 뜻함\n",
        "- tiny, small, medium, large 등의 학습된 모델 종류를 지정해서 모델을 로드"
      ],
      "metadata": {
        "id": "DTEf2deBCVq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# 한국어 고정이기 때문에 언어를 고정해주는 것이 좋음\n",
        "model.generation_config.language = \"korean\"\n",
        "\n",
        "# 한국어 전사 태스크임을 명시\n",
        "# 프로세서에서 명시해줬지만 모델에서도 다시 명시해주는게 좋은 듯\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "# 디폴트값인 any는 디코더가 다국어에 맞는 토큰을 자동으로 찾도록 함\n",
        "# 한국어만 보면 되므로 None으로 지정해서 좀 더 정확성을 올려줌\n",
        "model.config.forced_decoder_ids = None\n",
        "\n",
        "# 문장 생성 중 억제되는 토큰이 없도록 리스트를 비워줌\n",
        "# 정확한 의미는 모르겠음..\n",
        "# suppress_tokens는 일종의 불용어 사전 같은 용도\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "SZXFYNhoCWj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Define the Training Arguments"
      ],
      "metadata": {
        "id": "39S850QKCYi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 최종 학습을 위한 파라미터 설정\n",
        "- 에포크 횟수, 모델 저장 경로 등등"
      ],
      "metadata": {
        "id": "2uinociuCZGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "\n",
        "# 파라미터 설정\n",
        "# 너무 많다..\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/whisper/save_model\",  # 모델의 최종 결과(가중치 테이블)이 저장될 곳\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    num_train_epochs = 10,\n",
        "    # max_steps=4000,  # epoch 대신 설정, Test할 때만 사용하는 파라미터\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True, # Cuda 사용 못하면 False로 둬야함\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False, # 허브에 업로드할지 여부\n",
        ")\n",
        "\n",
        "# 트레이너 설정\n",
        "# 위에서 나온 모든 객체들을 이용해서 트레이닝\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args, # 위에서 설정한 파라미터 적용\n",
        "    model=model,\n",
        "    train_dataset=low_call_voices_prepreocessed[\"train\"],\n",
        "    eval_dataset=low_call_voices_prepreocessed[\"valid\"],  # or \"test\"\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "1zqEN8OnCada"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. 학습 진행"
      ],
      "metadata": {
        "id": "QzrD6_MmCcrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습 진행\n",
        "- 진행 후 바로 저장\n",
        "- 리소스가 있어야 하지.."
      ],
      "metadata": {
        "id": "YXZRe5iuCd6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 탕진잼!!\n",
        "trainer.train()\n",
        "\n",
        "# 모델, 프로세서 저장 (토크나이저는 프로세서 안에 있으니까 따로 저장안함)\n",
        "trainer.save_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")\n",
        "processor.save_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")"
      ],
      "metadata": {
        "id": "Pa6umV9JCfUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가진행\n",
        "\n"
      ],
      "metadata": {
        "id": "BKmTXnLwChHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 과정은 train, vaild를 이용한 훈련과 검증 과정\n",
        "- 평가는 Test를 사용해 진행"
      ],
      "metadata": {
        "id": "VHN9W024CiJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# 저장한 모델과 프로세서 로드\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")\n",
        "processor = WhisperProcessor.from_pretrained(\"/content/drive/MyDrive/whisper/save_model/\")"
      ],
      "metadata": {
        "id": "yvC0smyzCjwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트레이너 셋팅. training_args는 훈련에서 사용한 것과 동일하게 사용\n",
        "# 여기서 train은 왜 집어넣는걸까..? 필요없는데..\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=low_call_voices_prepreocessed[\"train\"],\n",
        "    eval_dataset=low_call_voices_prepreocessed[\"test\"],  # for evaluation(not validation)\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "# 평가 진행\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "zBwDN0sqCkfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# 저장된 모델 불러와서 파이프라인에 넣어주면 됨\n",
        "pipe = pipeline(model=\"\")\n",
        "\n",
        "# 실제 사용 메소드는 이거 3줄이면 됨. 물론 오디오 전처리 코드는 따로.\n",
        "def transcribe(audio):\n",
        "\n",
        "    # 파이프라인에 오디오파일 넣어주면 모델이 텍스트로 변환해서 반환해줌\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "# 녹음하자마자 음성 파일로 따서 모델에 전달하고 싶다면 그라디오 사용\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Small Hindi\",\n",
        "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "DCgXk9bDClxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Whisper Fine Tuning"
      ],
      "metadata": {
        "id": "QmzhV2tDvz6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper 모델을 더 빠르고 효율적으로 사용하기 위한 기법.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fUvr3tR0CuBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter Efficient Fine-Tuning (PEFT)\n",
        "* 정의: PEFT는 모델의 일부 파라미터만 fine-tuning하는 기법입니다.\n",
        "* 목적: 적은 수의 파라미터만 fine-tuning하여 모델 성능을 유지하면서도 메모리와 계산 비용을 줄이는 것이 목적입니다.\n",
        "* 장점:\n",
        "적은 수의 파라미터 fine-tuning: 전체 모델을 fine-tuning하는 것보다 적은 수의 파라미터만 fine-tuning합니다.\n",
        "* 메모리 및 계산 비용 감소: 적은 수의 파라미터만 fine-tuning하므로 메모리와 계산 비용이 줄어듭니다.\n",
        "* 기법:\n",
        "Prompt Tuning\n",
        "LoRA (Low-Rank Adaptation)\n",
        "Prefix Tuning\n",
        "Adapter Tuning\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w7jb3I-HCwZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA (Low Rank Adaptation)\n",
        "* LoRA는 모델의 전체 파라미터를 fine-tuning하는 대신, 일부 파라미터만 업데이트하는 기법입니다.\n",
        "* 이를 통해 모델 크기를 크게 줄이면서도 성능 저하를 최소화할 수 있습니다.\n",
        "* LoRA는 GPT-3 175B 모델 대비 약 10,000배 적은 파라미터로 fine-tuning이 가능하다고 합니다.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8bxPbBmkCyg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Quantization (양자화)\n",
        "* 정의: 양자화는 모델의 가중치와 활성화 함수 값을 낮은 비트 수로 표현하는 기술입니다.\n",
        "* 목적: 모델 크기를 줄이고 추론 속도를 높이는 것이 목적입니다.\n",
        "* 장점:\n",
        " * 모델 크기 감소: 낮은 비트 수로 표현하면 모델 크기가 줄어듭니다.\n",
        " * 추론 속도 향상: 낮은 비트 수 연산이 빠르기 때문에 추론 속도가 빨라집니다.\n",
        "* 종류:\n",
        "정적 양자화: 모델 학습 후 가중치를 양자화하는 방식\n",
        "* 동적 양자화: 추론 시 입력 데이터에 따라 동적으로 양자화하는 방식\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GB3Bt-ixCu3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 왜 Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)를 사용해야 되는가?\n"
      ],
      "metadata": {
        "id": "WB8VhPw5C28T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PEFT\n",
        "    * 효과적으로 parameter를 줄여서 fine tuning 속도 개선\n",
        "    * 목적: 병목 현상을 해결\n",
        "    * 접근법(예: 저수준 적응): 사전 훈련된 모델의 대부분의 매개변수를 동결시키면서 추가 모델 매개변수의 일부만 미세 조정하여 계산 및 저장 비용 크게 줄임\n",
        "        * 대규모 모델의 전체 미세 조정 중 관찰되는 catastrophic forgetting 문제를 극복할 수 있음\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dRUsC1ZtC6hE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 사이즈 증가로 fine tuning하는 것이 계산 복잡성 증가와 메모리 사용량 증가\n",
        "    * 예를 들어, Whisper-large-v2 모델을 완전한 미세 조정을 위해 약 24GB의 GPU VRAM이 필요하며, 각 미세 조정된 모델은 약 7GB의 저장 공간을 필요함\n",
        "\n",
        "    * 제한적인 환경에서 bottleneck 발생하고 원하는 결과를 얻기 힘듦\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mD2lIjtxC7ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoRA가 무엇인가?"
      ],
      "metadata": {
        "id": "GuXerhGBC-kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PEFT에서 여러 매개변수 효율적인 기술을 기본으로 제공\n",
        "    * 그 중 하나인 Low Rank Adaptation (LoRA)\n",
        "        * 사전 훈련된 모델 가중치를 동결하고 Transformer 아키텍처의 각 레이어에 훈련 가능한 랭크 분해 행렬을 삽입 (High Rank 즉 많은 연결이 되어 있는 것들보다 연결이 적은 Low Rank로 만들어서 계산량을 줄임)\n",
        "            * Downstream 작업에 대한 훈련 가능한 매개변수 수가 크게 감소\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Sfg41PyIC_q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 통계로 보는 PEFT 효과"
      ],
      "metadata": {
        "id": "Sg9LPzmYDBGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT 적용 모델\n",
        "\n",
        "    1. GPU VRAM이 8GB 미만인 환경에서 16억 개의 매개변수를 가진 모델을 미세 조정 🤯\n",
        "    2. 훨씬 적은 수의 훈련 가능한 매개변수를 사용하여 거의 5배 더 큰 배치 크기를 사용 가능 📈\n",
        "    3. 생성된 체크포인트는 원본 모델의 크기의 1%인 약 60MB 🚀\n",
        "\n",
        "* 기존 🤗 transformers Whisper에서 변형이 많이 되지 않았음\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "v_H4g8bvDCXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 환경설정"
      ],
      "metadata": {
        "id": "_EQtYVMjDETt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 Whisper 모델을 미세 조정하기 위해 몇 가지 인기 있는 Python 패키지를 사용할 것입니다.\n",
        "\n",
        "데이터 세트를 사용하여 훈련 데이터를 다운로드하고 준비하며 변환기를 사용하여 Whisper 모델을 로드하고 훈련할 것입니다.\n",
        "\n",
        "또한 오디오 파일을 전처리하고 모델 성능을 평가하기 위해 librosa 패키지가 필요합니다.\n",
        "\n",
        "마지막으로 PEFT, 비트앤바이트, 가속을 사용하여 LoRA로 모델을 준비하고 미세 조정합니다."
      ],
      "metadata": {
        "id": "oYD5Agx9DFLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PEFT 라이브러리 설치\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main\n",
        "# peft: 허깅페이스에서 개발한 Parameter Efficient Fine-Tuning 라이브러리\n",
        "# 이 라이브러리를 통해 모델의 일부 파라미터만 fine-tuning할 수 있어 메모리와 계산 비용을 줄일 수 있습니다.\n",
        "\n",
        "# 이렇게 필요한 라이브러리들을 설치하면 LoRA와 PEFT를 사용하여 Whisper 모델을 효율적으로 fine-tuning할 수 있습니다.\n",
        "# 이를 통해 모델 크기를 줄이고, 추론 속도를 높이며, 메모리와 계산 비용을 절감할 수 있습니다."
      ],
      "metadata": {
        "id": "cJ2-izZIDGmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n",
        "# transformers: 허깅페이스의 언어 모델 라이브러리\n",
        "# datasets: 허깅페이스의 데이터셋 라이브러리\n",
        "# librosa: 오디오 처리 라이브러리\n",
        "# evaluate: 모델 평가 도구\n",
        "# jiwer: 음성 인식 성능 평가 지표\n",
        "# gradio: 웹 기반 UI 라이브러리\n",
        "# bitsandbytes==0.37: 비트 연산 가속화 라이브러리, 버전 0.37 사용\n",
        "# accelerate: 모델 학습 가속화 라이브러리"
      ],
      "metadata": {
        "id": "xIm4t28lDHxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 환경이 설정되었으므로 Colab에 적합한 GPU를 확보해 보겠습니다!\n",
        "\n",
        "안타깝게도 Google Colab 무료 버전으로 좋은 GPU에 액세스하는 것이 훨씬 더 어려워지고 있습니다.\n",
        "\n",
        "그러나 Google Colab Pro를 사용하면 V100 또는 P100 GPU를 할당하는 데 문제가 없습니다.\n",
        "\n",
        "GPU를 얻으려면 런타임 -> 런타임 유형 변경을 클릭한 다음 하드웨어 가속기를 없음에서 GPU로 변경하세요.\n",
        "\n",
        "GPU가 할당되었는지 확인하고 해당 사양을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "-2bpdnplDJFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NVIDIA GPU 정보를 확인하는 명령어 실행\n",
        "gpu_info = !nvidia-smi\n",
        "\n",
        "# 명령어 실행 결과를 문자열로 변환\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "\n",
        "# GPU 정보에 'failed'가 포함되어 있는지 확인\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  # GPU에 연결되어 있지 않은 경우 출력\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  # GPU 정보 출력\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "# !nvidia-smi 명령어를 실행하여 NVIDIA GPU 정보를 확인합니다.\n",
        "# 이 명령어는 GPU 드라이버 버전, CUDA 버전, GPU 온도, 팬 속도 등 다양한 정보를 제공합니다.\n",
        "# gpu_info = '\\n'.join(gpu_info) 코드를 통해 명령어 실행 결과를 문자열로 변환합니다.\n",
        "# if gpu_info.find('failed') >= 0: 코드에서는 GPU 정보 문자열에 'failed'가 포함되어 있는지 확인합니다. 이는 GPU에 연결되어 있지 않은 경우 발생할 수 있습니다.\n",
        "# 'failed'가 포함되어 있는 경우 \"Not connected to a GPU\"라는 메시지를 출력합니다.\n",
        "# 'failed'가 포함되어 있지 않은 경우 GPU 정보를 출력합니다.\n",
        "\n",
        "# 이 코드를 통해 사용자는 자신의 시스템에 NVIDIA GPU가 연결되어 있는지, 그리고 GPU 정보를 확인할 수 있습니다.\n",
        "# 이는 딥러닝 모델 학습 및 추론 시 GPU 활용을 위해 필수적인 정보입니다."
      ],
      "metadata": {
        "id": "tEDRFPSPDLW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab에서 제공한 GPU를 사용하도록 환경을 구성해 보겠습니다."
      ],
      "metadata": {
        "id": "5l6FDOGODNfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# CUDA_VISIBLE_DEVICES 환경 변수를 \"0\"으로 설정\n",
        "# 이는 CUDA를 사용하는 프로그램이 첫 번째 GPU(일반적으로 0부터 시작)만 사용하도록 지정하는 것입니다.\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "# import os를 통해 운영 체제와 상호 작용하기 위한 os 모듈을 가져옵니다.\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" 코드를 통해 CUDA_VISIBLE_DEVICES 환경 변수를 0으로 설정합니다.\n",
        "# 이는 CUDA를 사용하는 프로그램이 첫 번째 GPU(일반적으로 0부터 시작)만 사용하도록 지정하는 것입니다.\n",
        "\n",
        "# 이 코드를 통해 사용자는 CUDA를 사용하는 프로그램이 특정 GPU만을 활용하도록 설정할 수 있습니다.\n",
        "# GPU가 여러 개인 경우, 이러한 설정을 통해 각 프로그램이 특정 GPU를 사용하도록 제어할 수 있습니다."
      ],
      "metadata": {
        "id": "nIIdzJUrDOgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hugging Face Hub](https://huggingface.co/)에 모델 체크포인트를 직접 업로드하는 것이 좋습니다.\n",
        "훈련하는 동안. 허브는 다음을 제공합니다.\n",
        "- 통합 버전 제어: 학습 중에 모델 체크포인트가 손실되지 않도록 할 수 있습니다.\n",
        "- Tensorboard 로그: 훈련 과정에서 중요한 측정항목을 추적합니다.\n",
        "- 모델 카드: 모델이 수행하는 작업과 의도된 사용 사례를 문서화합니다.\n",
        "- 커뮤니티: 커뮤니티와 쉽게 공유하고 협업할 수 있는 방법입니다!\n",
        "\n",
        "노트북을 허브에 연결하는 것은 간단합니다.\n",
        "\n",
        "메시지가 표시되면 허브 인증 토큰을 입력하기만 하면 됩니다.\n",
        "\n",
        "Hub 인증 토큰을 [여기](https://huggingface.co/settings/tokens)에서 찾으세요."
      ],
      "metadata": {
        "id": "a7_Uxkp5DPiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface_hub 라이브러리에서 notebook_login 함수를 가져옵니다.\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login() 함수를 호출하여 Hugging Face Hub에 로그인합니다.\n",
        "# 이 함수는 로그인 토큰을 입력하는 팝업 창을 띄워줍니다.\n",
        "notebook_login()\n",
        "\n",
        "# from huggingface_hub import notebook_login을 통해 Hugging Face Hub 라이브러리에서 notebook_login 함수를 가져옵니다.\n",
        "# notebook_login() 함수를 호출하면 Hugging Face Hub에 로그인할 수 있습니다.\n",
        "# 이 함수는 로그인 토큰을 입력하는 팝업 창을 띄워줍니다.\n",
        "\n",
        "# 이 코드를 실행하면 Hugging Face Hub에 로그인할 수 있습니다.\n",
        "# 로그인 토큰을 직접 입력하는 방법 외에도 huggingface_hub 라이브러리를 사용하여 로그인할 수 있습니다.\n",
        "# 이를 통해 Hugging Face Hub의 다양한 기능을 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "MRYIzXmxDRXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로 Whisper 모델 체크포인트와 작업 세부정보를 정의합니다."
      ],
      "metadata": {
        "id": "AJbu17tODS0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "\n",
        "# 수행할 작업을 \"transcribe\"로 설정합니다.\n",
        "# 이는 오디오 파일을 텍스트로 전사하는 작업을 의미합니다.\n",
        "task = \"transcribe\"\n",
        "\n",
        "\n",
        "# model_name_or_path = \"openai/whisper-large-v2\"에서는 사용할 Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "# 이 경우 \"openai/whisper-large-v2\" 모델을 사용합니다.\n",
        "# task = \"transcribe\"에서는 수행할 작업을 \"transcribe\"로 설정합니다.\n",
        "# 이는 오디오 파일을 텍스트로 전사하는 작업을 의미합니다.\n",
        "\n",
        "# 이 코드는 Whisper 모델을 사용하여 오디오 파일을 텍스트로 전사하는 작업을 수행할 수 있도록 설정합니다.\n",
        "#  이를 통해 음성 인식 및 자동 전사 기능을 구현할 수 있습니다."
      ],
      "metadata": {
        "id": "jrg7ySzoDSoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 Whisper를 미세 조정하려는 언어를 포함하여 데이터 세트 세부 사항을 정의합니다."
      ],
      "metadata": {
        "id": "lXGfYbd8DVdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Voice 데이터셋의 이름을 지정합니다.\n",
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "\n",
        "# 사용할 언어를 한국어로 설정합니다.\n",
        "language = \"korean\"\n",
        "\n",
        "# 한국어의 ISO 639-1 언어 코드인 \"ko\"를 사용합니다.\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune\n",
        "\n",
        "\n",
        "# dataset_name = \"mozilla-foundation/common_voice_13_0\"에서는 사용할 Common Voice 데이터셋의 이름을 지정합니다.\n",
        "# 이 경우 \"mozilla-foundation/common_voice_13_0\" 데이터셋을 사용합니다.\n",
        "# language = \"korean\"에서는 사용할 언어를 한국어로 설정합니다.\n",
        "# language_abbr = \"ko\"에서는 한국어의 ISO 639-1 언어 코드인 \"ko\"를 사용합니다.\n",
        "# 이는 데이터셋에서 한국어 데이터를 선택하는 데 사용됩니다."
      ],
      "metadata": {
        "id": "RwgkGv88DWKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 로드\n"
      ],
      "metadata": {
        "id": "srzppdo7DX49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🤗 데이터 세트를 사용하면 데이터를 다운로드하고 준비하는 것이 매우 간단합니다.\n",
        "\n",
        "단 한 줄의 코드로 Common Voice 스플릿을 다운로드하고 준비할 수 있습니다.\n",
        "\n",
        "먼저 Hugging Face Hub(mozilla-foundation/common_voice_13_0)의 사용 약관에 동의했는지 확인하세요.\n",
        "\n",
        "약관에 동의하면 데이터 세트에 대한 전체 액세스 권한을 갖고 데이터를 로컬로 다운로드할 수 있습니다."
      ],
      "metadata": {
        "id": "lhZBtvdUDYy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Common Voice 데이터셋을 로드하기 위한 DatasetDict 객체를 생성합니다.\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "# load_dataset() 함수를 사용하여 Common Voice 데이터셋의 \"train+validation\" 데이터를 로드합니다.\n",
        "# 이때 language_abbr 변수를 사용하여 한국어 데이터를 선택합니다.\n",
        "# use_auth_token=True를 설정하여 Hugging Face API 토큰을 사용합니다.\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)\n",
        "\n",
        "# load_dataset() 함수를 사용하여 Common Voice 데이터셋의 \"test\" 데이터를 로드합니다.\n",
        "# 이때 language_abbr 변수를 사용하여 한국어 데이터를 선택합니다.\n",
        "# use_auth_token=True를 설정하여 Hugging Face API 토큰을 사용합니다.\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)\n",
        "\n",
        "# 로드된 데이터셋을 출력합니다.\n",
        "print(common_voice)\n",
        "\n",
        "\n",
        "\n",
        "# from datasets import load_dataset, DatasetDict에서는 Common Voice 데이터셋을 로드하기 위해 필요한 함수와 클래스를 가져옵니다.\n",
        "# common_voice = DatasetDict()에서는 Common Voice 데이터셋을 저장할 DatasetDict 객체를 생성합니다.\n",
        "# common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", use_auth_token=True)에서는 load_dataset() 함수를 사용하여 Common Voice 데이터셋의 \"train+validation\" 데이터를 로드합니다.\n",
        "# language_abbr 변수를 사용하여 한국어 데이터를 선택하며, use_auth_token=True를 설정하여 Hugging Face API 토큰을 사용합니다.\n",
        "# common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", use_auth_token=True)에서는 load_dataset() 함수를 사용하여 Common Voice 데이터셋의 \"test\" 데이터를 로드합니다.\n",
        "# 마찬가지로 language_abbr 변수를 사용하여 한국어 데이터를 선택하며, use_auth_token=True를 설정하여 Hugging Face API 토큰을 사용합니다.\n",
        "# print(common_voice)에서는 로드된 데이터셋을 출력합니다.\n",
        "\n",
        "# 이 코드를 통해 Common Voice 데이터셋의 한국어 데이터를 \"train+validation\"과 \"test\" 데이터로 분리하여 로드할 수 있습니다.\n",
        "#  이후 이 데이터를 사용하여 한국어 음성 인식 모델을 fine-tuning할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "nxHsXxE4DZt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 일반적인 ASR(음성 인식) 데이터셋\n",
        "    * 입력 오디오 샘플(오디오)과 해당되는 텍스트(문장)만 제공\n",
        "* Common Voice\n",
        "    * ASR에는 필요하지 않은 악센트와 로케일과 같은 추가 메타데이터 정보가 포함\n",
        "    * 일반적인 용도로 사용하고 미세 조정을 고려하기 위해 메타데이터 정보 무시"
      ],
      "metadata": {
        "id": "1P0GrVxpDbZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns( #필요 없는 컬럼 제거\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "7dbibCTZDcdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 특성 추출기(Feature Extractor), 토크나이저(Tokenizer), 그리고 데이터준비\n"
      ],
      "metadata": {
        "id": "aXqckv9WDdWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ASR 파이프라인은 세 단계로 분해될 수 있습니다.\n",
        "\n",
        "1. 원시 오디오 입력을 전처리하는 특징 추출기\n",
        "2. 시퀀스 간 매핑을 수행하는 모델\n",
        "3. 모델 출력을 텍스트 형식으로 후처리하는 토크나이저\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)와 [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)로 구성\n",
        "\n"
      ],
      "metadata": {
        "id": "KwjclFRhDe3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "# WhisperFeatureExtractor 클래스를 사용하여 Whisper 모델에 필요한 feature extractor를 로드합니다.\n",
        "# from_pretrained() 메서드를 사용하여 사전 학습된 feature extractor를 가져옵니다.\n",
        "# 이때 model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "\n",
        "\n",
        "# from transformers import WhisperFeatureExtractor에서는 Whisper 모델에 필요한 feature extractor를 가져옵니다.\n",
        "# feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)에서는 WhisperFeatureExtractor 클래스의 from_pretrained() 메서드를 사용하여 사전 학습된 feature extractor를 로드합니다.\n",
        "# model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "\n",
        "# 이 코드를 통해 Whisper 모델을 사용하기 위해 필요한 feature extractor를 로드할 수 있습니다.\n",
        "# 이후 이 feature extractor를 사용하여 음성 데이터를 Whisper 모델의 입력 형식으로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "0MdpPZKRDgnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "# WhisperTokenizer 클래스를 사용하여 Whisper 모델에 필요한 tokenizer를 로드합니다.\n",
        "# from_pretrained() 메서드를 사용하여 사전 학습된 tokenizer를 가져옵니다.\n",
        "# 이때 model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "# language 변수에는 사용할 언어를 지정하고, task 변수에는 수행할 작업(예: 음성 인식, 텍스트 생성 등)을 지정합니다.\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "\n",
        "\n",
        "# from transformers import WhisperTokenizer에서는 Whisper 모델에 필요한 tokenizer를 가져옵니다.\n",
        "# tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)에서는 WhisperTokenizer 클래스의 from_pretrained() 메서드를 사용하여 사전 학습된 tokenizer를 로드합니다.\n",
        "#  model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정하고, language 변수에는 사용할 언어를, task 변수에는 수행할 작업(예: 음성 인식, 텍스트 생성 등)을 지정합니다.\n",
        "\n",
        "\n",
        "# 이 코드를 통해 Whisper 모델을 사용하기 위해 필요한 tokenizer를 로드할 수 있습니다.\n",
        "#  이후 이 tokenizer를 사용하여 입력 데이터를 Whisper 모델의 형식으로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "W6blRQ-2DicP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기능 추출기와 토크나이저 사용을 단순화하기 위해 두 가지를 하나의 `WhisperProcessor` 클래스로 _wrap_할 수 있습니다.\n",
        "\n",
        " 이 프로세서 객체는 필요에 따라 오디오 입력 및 모델 예측에 사용될 수 있습니다.\n",
        "\n",
        "그렇게 하면 훈련 중에 두 가지 객체만 추적하면 됩니다.\n",
        "\n",
        "'프로세서' 및 '모델':"
      ],
      "metadata": {
        "id": "pH-HpN1NDjhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "# WhisperProcessor 클래스를 사용하여 Whisper 모델에 필요한 processor를 로드합니다.\n",
        "# from_pretrained() 메서드를 사용하여 사전 학습된 processor를 가져옵니다.\n",
        "# 이때 model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정합니다.\n",
        "# language 변수에는 사용할 언어를 지정하고, task 변수에는 수행할 작업(예: 음성 인식, 텍스트 생성 등)을 지정합니다.\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "\n",
        "\n",
        "# from transformers import WhisperProcessor에서는 Whisper 모델에 필요한 processor를 가져옵니다.\n",
        "# processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)에서는 WhisperProcessor 클래스의 from_pretrained() 메서드를 사용하여 사전 학습된 processor를 로드합니다.\n",
        "# model_name_or_path 변수에는 Whisper 모델의 이름 또는 경로를 지정하고, language 변수에는 사용할 언어를, task 변수에는 수행할 작업(예: 음성 인식, 텍스트 생성 등)을 지정합니다.\n",
        "\n",
        "\n",
        "# 이 코드를 통해 Whisper 모델을 사용하기 위해 필요한 processor를 로드할 수 있습니다.\n",
        "# 이후 이 processor를 사용하여 입력 데이터를 Whisper 모델의 형식으로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "Ei9lYYhvDkVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 데이터 준비\n"
      ],
      "metadata": {
        "id": "ijqXqOGvDlc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Voice 데이터셋의 첫 번째 예제를 출력하여 데이터의 형식을 살펴봄"
      ],
      "metadata": {
        "id": "etez1-mTDmrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "GeZl1_LUDnkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델 샘플링\n",
        "    * 입력 오디오는 48 kHz 새플링\n",
        "    * Whisper feature extractor에 전달하기 위해서 16 kHz로 다운샘플 진행\n",
        "* 샘플링 속도 설정\n",
        "    * Dataset의 [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) 방법 사용: 오디오 입력을 올바른 샘플링 속도로 설정\n",
        "    * 오디오를 변경하는 것이 아니라 오디오 샘플을 실시간으로 받을 수 있도록 함\n"
      ],
      "metadata": {
        "id": "yFOZhcU_DogF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "# common_voice 데이터셋의 \"audio\" 열을 Audio 객체로 변환합니다.\n",
        "# Audio 객체는 오디오 데이터를 처리하는 데 필요한 정보를 포함하고 있습니다.\n",
        "# sampling_rate=16000 옵션을 통해 오디오 데이터의 샘플링 레이트를 16kHz로 설정합니다.\n",
        "# 이는 일반적인 음성 처리 작업에 적합한 샘플링 레이트입니다.\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "\n",
        "# from datasets import Audio에서는 오디오 데이터를 처리하는 데 필요한 Audio 클래스를 가져옵니다.\n",
        "# common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))에서는 common_voice 데이터셋의 \"audio\" 열을 Audio 객체로 변환합니다. sampling_rate=16000 옵션을 통해 오디오 데이터의 샘플링 레이트를 16kHz로 설정합니다.\n",
        "# 이는 일반적인 음성 처리 작업에 적합한 샘플링 레이트입니다.\n",
        "\n",
        "\n",
        "# 이 코드를 통해 common_voice 데이터셋의 오디오 데이터를 처리할 수 있습니다.\n",
        "# 이후 이 데이터를 사용하여 음성 인식, 음성 합성 등의 작업을 수행할 수 있습니다.\n",
        "# 또한 사용자 정의 오디오 데이터셋을 생성하고 공유할 수 있는 기능도 제공합니다.\n",
        "# 이를 통해 음성 처리 모델 개발에 필요한 데이터를 쉽게 확보할 수 있습니다."
      ],
      "metadata": {
        "id": "PmG7zK9HDpi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common Voice 데이터세트의 첫 번째 오디오 샘플을 다시 로드하면 리샘플링됩니다.\n",
        "원하는 샘플링 속도로 설정합니다."
      ],
      "metadata": {
        "id": "68KjXN4_DsUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "ZIU3ZTNODuNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델에 사용할 데이터를 준비하는 함수를 작성할 수 있습니다.\n",
        "1. `batch[\"audio\"]`를 호출하여 오디오 데이터를 로드하고 리샘플링합니다. 위에서 설명한 대로 🤗 데이터세트는 필요한 모든 리샘플링 작업을 즉시 수행합니다.\n",
        "2. 특징 추출기를 사용하여 1차원 오디오 배열에서 log-Mel 스펙트로그램 입력 특징을 계산합니다.\n",
        "3. 토크나이저를 사용하여 텍스트를 라벨 ID로 인코딩합니다."
      ],
      "metadata": {
        "id": "MJycSnwlDwDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # 1. 오디오 데이터 로드 및 16kHz로 리샘플링\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # 2. 입력 오디오 배열에서 로그-멜 스펙트로그램 특징 추출\n",
        "    # feature_extractor는 오디오 특징 추출기 모델\n",
        "    # input_features는 추출된 특징 벡터\n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # 3. 타겟 문장을 토크나이저를 사용하여 라벨 ID로 인코딩\n",
        "    # tokenizer는 텍스트를 토큰화하고 ID로 변환하는 모델\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
        "\n",
        "    # 4. 준비된 데이터를 반환\n",
        "    return batch\n",
        "\n",
        "\n",
        "# audio = batch[\"audio\"]에서는 입력 데이터셋의 \"audio\" 필드에서 오디오 데이터를 가져옵니다.\n",
        "# batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]에서는 입력 오디오 배열에서 로그-멜 스펙트로그램 특징을 추출합니다. feature_extractor는 오디오 특징 추출기 모델이며, 추출된 특징 벡터는 input_features에 저장됩니다.\n",
        "# batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids에서는 타겟 문장을 토크나이저를 사용하여 라벨 ID로 인코딩합니다.\n",
        "# tokenizer는 텍스트를 토큰화하고 ID로 변환하는 모델입니다.\n",
        "# 준비된 데이터는 return batch를 통해 반환됩니다.\n",
        "\n",
        "# 이 코드는 오디오 데이터와 텍스트 데이터를 처리하여 모델 학습에 사용할 수 있는 형태로 변환합니다.\n",
        "# 오디오 데이터는 로그-멜 스펙트로그램 특징으로 변환되며, 텍스트 데이터는 토크나이저를 통해 라벨 ID로 인코딩됩니다."
      ],
      "metadata": {
        "id": "f9I9TsvhDxdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 세트의 `.map` 메소드를 사용하여 모든 훈련 예제에 데이터 준비 기능을 적용할 수 있습니다.\n",
        "\n",
        "'num_proc' 인수는 사용할 CPU 코어 수를 지정합니다.\n",
        "\n",
        "`num_proc` > 1로 설정하면 다중 처리가 활성화됩니다.\n",
        "\n",
        "`.map` 메서드가 다중 처리로 인해 중단되면 `num_proc=1`을 설정하고 데이터세트를 순차적으로 처리합니다.\n",
        "\n",
        "직접 만들어 보세요 🍵, 데이터세트 크기에 따라 20~30분 정도 걸릴 수 있습니다 ⏰"
      ],
      "metadata": {
        "id": "4HymkB3XDyoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Voice 데이터셋을 처리하는 코드\n",
        "common_voice = common_voice.map(\n",
        "    # prepare_dataset 함수를 적용하여 데이터셋을 준비\n",
        "    prepare_dataset,\n",
        "    # 기존 \"train\" 데이터셋의 열을 제거\n",
        "    remove_columns=common_voice.column_names[\"train\"],\n",
        "    # 2개의 프로세스를 사용하여 병렬 처리\n",
        "    num_proc=2,\n",
        ")\n",
        "\n",
        "\n",
        "# common_voice = common_voice.map(...): Common Voice 데이터셋을 처리하는 코드입니다.\n",
        "# prepare_dataset: 이전에 정의한 데이터셋 준비 함수를 적용합니다.\n",
        "# remove_columns=common_voice.column_names[\"train\"]: \"train\" 데이터셋의 열을 제거합니다. 이는 중복된 정보를 제거하기 위함입니다.\n",
        "# num_proc=2: 2개의 프로세스를 사용하여 병렬 처리를 수행합니다. 이를 통해 처리 속도를 높일 수 있습니다.\n",
        "\n",
        "# 이 코드는 Common Voice 데이터셋을 처리하여 모델 학습에 사용할 수 있는 형태로 변환합니다.\n",
        "#  데이터셋의 오디오 데이터는 로그-멜 스펙트로그램 특징으로 변환되며, 텍스트 데이터는 토크나이저를 통해 라벨 ID로 인코딩됩니다.\n",
        "#  또한 병렬 처리를 통해 처리 속도를 높이고 있습니다."
      ],
      "metadata": {
        "id": "VXcfPHD4Dz5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "gDpkwdFwD1ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 훈련 및 검증\n"
      ],
      "metadata": {
        "id": "f2oQJ5SeD2E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 파이프라인\n",
        "\n",
        "* [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)가 대부분의 작업을 처리:\n",
        "\n",
        "\n",
        "1. 데이터 collator 정의: 데이터 콜레이터는 우리가 전처리한 데이터를 가져와 모델에 사용할 수 있는 PyTorch 텐서로 준비\n",
        "2. 평가 지표: 평가 중에는 모델을 글자 오류율  [word error rate (CER)](https://huggingface.co/metrics/cer)지표를 사용하여 평가\n",
        "3. 사전 훈련된 체크포인트 load: 사전 훈련된 체크포인트를 로드하고 훈련을 위해 올바르게 구성\n",
        "4. 훈련 구성 정의: 🤗 Trainer가 훈련 스케줄을 정의에 사용\n",
        "\n",
        "* 미세 조정한 후에는 테스트 데이터에서 모델을 평가하여 한국어 음성을 올바르게 전사"
      ],
      "metadata": {
        "id": "VXzpnliHD3d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Collator 정의"
      ],
      "metadata": {
        "id": "vkcFpAnGD4vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* 시퀀스-투-시퀀스 음성 모델의 데이터 콜레이터\n",
        "    * Input_features와 labels를 독립적으로 처리\n",
        "    \n",
        "    - Input_features: feature extractor에 의해 처리\n",
        "    - labels: tokenizer에 의해 처리\n",
        "\n",
        "* Input_features는 이미 30초로 패딩되어 있고 특성 추출기에 의해 고정된 차원의 로그 멜 스펙트로그램으로 변환. 따라서 우리가 해야 할 일은 input_features를 배치 처리된 PyTorch 텐서로 변환\n",
        "\n",
        "* labels는 패딩되지 않음 먼저 배치 내에서 최대 길이에 맞게 시퀀스를 패딩하고, tokenizer의 .pad 메서드를 사용하여 시퀀스를 패딩 패딩 토큰은 손실을 계산할 때 고려되지 않도록 -100으로 대체. 그런 다음 레이블 시퀀스의 시작에서 BOS 토큰을 잘라서 훈련 중에 나중에 이를 추가\n",
        "\n",
        "* 이전에 정의한 WhisperProcessor를 활용하여 특성 추출기 및 토크나이저 작업을 모두 수행 가능"
      ],
      "metadata": {
        "id": "XMCZd8y8D5yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # 1. 입력 특징과 라벨 데이터를 분리\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # 2. 입력 특징 배치 생성\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 3. 라벨 데이터 배치 생성 및 패딩\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 4. 패딩된 라벨에서 -100으로 마스킹하여 손실 계산 시 패딩 부분 무시\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # 5. 이전 토크나이징 과정에서 BOS 토큰이 추가된 경우 제거\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        # 6. 처리된 배치 데이터 반환\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "\n",
        "# input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]: 입력 특징과 라벨 데이터를 분리합니다.\n",
        "# batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\"): 입력 특징 배치를 생성합니다. self.processor.feature_extractor.pad()는 입력 특징을 패딩하여 배치 형태로 변환합니다.\n",
        "# labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\"): 라벨 데이터 배치를 생성합니다. self.processor.tokenizer.pad()는 라벨 데이터를 패딩하여 배치 형태로 변환합니다.\n",
        "# labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100): 패딩된 라벨 데이터에서 -100으로 마스킹하여 손실 계산 시 패딩 부분을 무시합니다.\n",
        "# if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item(): labels = labels[:, 1:]: 이전 토크나이징 과정에서 BOS(Begin Of Sequence) 토큰이 추가된 경우 제거합니다.\n",
        "# batch[\"labels\"] = labels; return batch: 처리된 배치 데이터를 반환합니다.\n",
        "\n",
        "\n",
        "# 이 코드는 음성 인식 모델 학습을 위한 데이터 전처리 과정을 담당합니다.\n",
        "# 입력 특징과 라벨 데이터를 분리하고, 각각 패딩하여 배치 형태로 변환합니다.\n",
        "# 또한 라벨 데이터에서 패딩 부분을 -100으로 마스킹하여 손실 계산 시 이를 무시하도록 합니다.\n",
        "# 마지막으로 BOS 토큰이 추가된 경우 제거하여 최종 배치 데이터를 반환합니다."
      ],
      "metadata": {
        "id": "ENkHifrDD7C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collator 초기화 진행"
      ],
      "metadata": {
        "id": "4tZ3YBVtD9Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DataCollatorSpeechSeq2SeqWithPadding 클래스 인스턴스 생성\n",
        "# processor 매개변수에는 음성 데이터 전처리를 위한 Processor 객체가 전달됩니다.\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "\n",
        "# data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor): DataCollatorSpeechSeq2SeqWithPadding 클래스의 인스턴스를 생성합니다.\n",
        "#  이 클래스는 음성 데이터 전처리를 담당하며, processor 매개변수에는 음성 데이터 전처리를 위한 Processor 객체가 전달됩니다.\n",
        "\n",
        "\n",
        "# 이 코드는 음성 인식 모델 학습을 위한 데이터 전처리 과정을 수행하는 부분입니다.\n",
        "#  DataCollatorSpeechSeq2SeqWithPadding 클래스는 입력 특징과 라벨 데이터를 분리하고, 각각 패딩하여 배치 형태로 변환합니다.\n",
        "#   또한 라벨 데이터에서 패딩 부분을 -100으로 마스킹하여 손실 계산 시 이를 무시하도록 합니다.\n",
        "#  마지막으로 BOS 토큰이 추가된 경우 제거하여 최종 배치 데이터를 반환합니다."
      ],
      "metadata": {
        "id": "6AdXt-8ED969"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 평가 지표"
      ],
      "metadata": {
        "id": "44NsxjO5D_kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ASR 시스템을 평가하기 위한 '사실상의' 지표인 한 단어 오류율(CER) 메트릭을 사용\n",
        "* 더 많은 정보는 [문서](https://huggingface.co/metrics/cer)를 참조. 우리는 🤗 Evaluate에서 CER 메트릭을 로드"
      ],
      "metadata": {
        "id": "z51FLfUBEArt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. evaluate 라이브러리 import\n",
        "import evaluate\n",
        "\n",
        "# 2. \"cer\" 메트릭 로드\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "\n",
        "# import evaluate: Hugging Face의 evaluate 라이브러리를 import합니다.\n",
        "# 이 라이브러리는 다양한 자연어 처리 및 음성 처리 평가 메트릭을 제공합니다.\n",
        "# metric = evaluate.load(\"cer\"): evaluate.load(\"cer\") 함수를 사용하여 Character Error Rate (CER) 메트릭을 로드합니다.\n",
        "# CER은 자동 음성 인식 시스템을 평가하는 데 사용되는 일반적인 메트릭으로, Word Error Rate (WER)과 유사합니다.\n",
        "\n",
        "# 추가 정보\n",
        "# CER (Character Error Rate)\n",
        "# CER은 자동 음성 인식 시스템의 성능을 평가하는 데 사용되는 메트릭입니다.\n",
        "# CER은 입력 문장과 인식 결과 간의 문자 단위 편집 거리(insertion, deletion, substitution)를 계산하여 오류율을 측정합니다.\n",
        "# CER은 WER과 유사하지만, 단어 단위가 아닌 문자 단위로 오류를 계산합니다.\n",
        "# CER은 특히 한국어와 같이 띄어쓰기가 중요한 언어에서 유용한 평가 지표가 될 수 있습니다."
      ],
      "metadata": {
        "id": "eZWK2MXNEBms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 사전 학습 모델 로드"
      ],
      "metadata": {
        "id": "s8feGwV1EDGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 사전 훈련된 Whisper 체크포인트를 로드\n",
        "    * 이 작업은 🤗 Transformers를 사용하여 매우 간단\n",
        "\n",
        "\n",
        "\n",
        "* 모델의 메모리 사용량을 줄이기 위해 모델을 8비트로         \n",
        "    * 모델을 1/4 정밀도(32비트와 비교했을 때)로 양자화하여 성능 손실을 최소화 [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ],
      "metadata": {
        "id": "ha_A5RPHEE4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade bitsandbytes"
      ],
      "metadata": {
        "id": "Umew6KhqED9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "# WhisperForConditionalGeneration 모델을 transformers 라이브러리에서 import합니다.\n",
        "# 이 모델은 조건부 생성(conditional generation) 작업을 위한 모델입니다.\n",
        "\n",
        "# Quantization(양자화)을 수행하여 weight들을 float32에서 int로 변환하여 성능을 최적화합니다.\n",
        "# 이를 통해 모델의 성능을 최적화할 수 있습니다.\n",
        "\n",
        "# WhisperForConditionalGeneration 모델을 불러오고, 8비트로 로드하며, 디바이스 매핑을 설정합니다.\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map={\"\":0})\n",
        "# model_name_or_path: 불러올 모델의 이름 또는 경로를 지정합니다.\n",
        "# load_in_8bit=True: 모델을 8비트로 로드하도록 설정합니다. 이는 양자화된 모델을 의미합니다.\n",
        "# device_map={\"\":0}: 모델을 특정 디바이스에 매핑하는 설정을 지정합니다. 여기서는 빈 문자열(\"\")을 0번 디바이스에 매핑하는 것으로 보입니다.\n"
      ],
      "metadata": {
        "id": "n2PANDjYEHrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 모델 후처리"
      ],
      "metadata": {
        "id": "fk4Fa2o8EKrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 훈련을 가능하게 하기 위해 8비트 모델에 몇 가지 후처리 단계를 적용\n",
        "2. 모델 레이어를 동결, 훈련과 모델의 안정성을 위해 레이어 정규화와 출력 레이어를 float32로 캐스팅\n",
        "\n",
        "(모델 안정성과 layer normalization 분석, float32로 캐스팅 하는 이유)"
      ],
      "metadata": {
        "id": "C40OGzKyEL1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "# from peft import prepare_model_for_kbit_training 명령어로 peft 라이브러리에서 prepare_model_for_kbit_training 함수를 불러옵니다.\n",
        "# 이 함수는 모델을 8비트 양자화(quantization)하여 성능을 최적화하는 데 사용됩니다.\n",
        "\n",
        "# 일반적으로 모델의 weight들은 float32 형식으로 저장되어 있습니다.\n",
        "# 이를 int 형식으로 변환하는 양자화 작업을 통해 모델의 성능을 최적화할 수 있습니다.\n",
        "# 양자화된 모델은 메모리 사용량이 줄어들고 추론 속도가 향상됩니다.\n",
        "\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "# prepare_model_for_kbit_training 함수는 peft 라이브러리에서 제공하는 함수입니다.\n",
        "# 이 함수는 입력으로 받은 모델 객체를 양자화된 상태로 변환하여 반환합니다.\n",
        "# 양자화된 모델 객체는 model 변수에 할당됩니다."
      ],
      "metadata": {
        "id": "deo3DuX7EU4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Whisper 모델은 인코더에 컨볼루션 레이어를 사용하기 때문에 체크포인팅은 grad 연산을 비활성. 이를 피하기 위해 입력을 특별히 trainable하게 만들어야 합니다.\n"
      ],
      "metadata": {
        "id": "qtFKea7dEWQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make_inputs_require_grad 함수 정의\n",
        "def make_inputs_require_grad(module, input, output):\n",
        "    # output 텐서의 requires_grad 속성을 True로 설정\n",
        "    # 이를 통해 출력 텐서에 대한 변화도(gradient)를 계산할 수 있게 됩니다.\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "\n",
        "# register_forward_hook 메서드를 사용하여 make_inputs_require_grad 함수를 conv1 레이어에 등록\n",
        "# 이렇게 하면 conv1 레이어의 forward 연산이 실행될 때마다 make_inputs_require_grad 함수가 호출됩니다.\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "\n",
        "# make_inputs_require_grad 함수는 모듈의 입력과 출력을 받아서 출력 텐서의 requires_grad 속성을 True로 설정합니다.\n",
        "# 이를 통해 출력 텐서에 대한 변화도(gradient)를 계산할 수 있게 됩니다.\n",
        "# register_forward_hook 메서드를 사용하여 make_inputs_require_grad 함수를 conv1 레이어에 등록합니다.\n",
        "# 이렇게 하면 conv1 레이어의 forward 연산이 실행될 때마다 make_inputs_require_grad 함수가 호출됩니다."
      ],
      "metadata": {
        "id": "_PaTfSW9EXEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Low-rank adapters (LoRA)를 모델에 적용\n"
      ],
      "metadata": {
        "id": "tb9t78f9EYYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* `PeftModel`을 로드하고 `peft`의 `get_peft_model` 유틸리티 함수를 사용하여 낮은 순위 어댑터(LoRA)를 사용할 것이라고 지정해 보겠습니다.\n"
      ],
      "metadata": {
        "id": "QwnPSf2mEZkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "# LoraConfig 객체를 생성하여 Lora 모델의 구성을 설정합니다.\n",
        "# r=32: Lora 모델의 랭크 값을 32로 설정합니다. 이는 모델의 복잡도를 결정합니다.\n",
        "# lora_alpha=64: Lora 모델의 스케일링 팩터를 64로 설정합니다.\n",
        "# target_modules=[\"q_proj\", \"v_proj\"]: Lora 모듈이 적용될 모델의 특정 레이어를 지정합니다.\n",
        "# lora_dropout=0.05: Lora 모델의 드롭아웃 비율을 0.05로 설정합니다.\n",
        "# bias=\"none\": Lora 모델의 편향을 \"none\"으로 설정합니다.\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "# get_peft_model 함수를 사용하여 PEFT 모델을 가져옵니다.\n",
        "# 이 함수는 기존 모델에 Lora 모듈을 적용하여 PEFT 모델을 생성합니다.\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# model.print_trainable_parameters()를 호출하여 훈련 가능한 매개변수를 출력합니다.\n",
        "# 이를 통해 모델의 구조와 매개변수 분포를 확인할 수 있습니다.\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "# 이 코드는 PEFT(Probabilistic Early-stopping Framework)를 사용하여 Lora 모델을 구성하고 훈련 가능한 매개변수를 출력하는 것입니다.\n",
        "# PEFT는 모델 훈련 중에 조기 종료를 결정하기 위한 확률적인 프레임워크입니다. Lora 모델은 모델의 성능을 향상시키기 위해 사용되는 기술 중 하나로, 모델의 일부 레이어에만 추가 매개변수를 도입하여 모델의 복잡도를 낮추고 훈련 속도를 높일 수 있습니다.\n",
        "# 이 코드는 PEFT를 사용하여 Lora 모델을 설정하고 훈련 가능한 매개변수를 출력하는 것입니다. 이를 통해 모델의 성능을 향상시키고 효율적인 훈련을 수행할 수 있습니다.\n",
        "# 추가로, 이 코드는 모델 구성과 매개변수 출력 외에도 모델 저장, 로드, 평가 등의 기능을 포함할 수 있습니다. 이를 통해 모델 개발 및 배포 과정을 더욱 효율적으로 수행할 수 있습니다."
      ],
      "metadata": {
        "id": "d6EBDiqEEaxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1%**의 학습 parameter를 사용하였고 **Parameter-Efficient Fine-Tuning**를 적용\n"
      ],
      "metadata": {
        "id": "dkLIroSrEcpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 훈련 구성 정의"
      ],
      "metadata": {
        "id": "cfv-HTBPEePG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "마지막 단계에서는 훈련과 관련된 모든 매개변수를 정의 훈련 인자에 대한 자세한 내용은 해당 문서를 참조 Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ],
      "metadata": {
        "id": "HVDnatmGEfqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Seq2SeqTrainingArguments 클래스는 sequence-to-sequence 모델 학습을 위한 다양한 하이퍼파라미터를 정의할 수 있게 해줍니다.\n",
        "# 이 클래스는 transformers 라이브러리에서 제공됩니다.\n",
        "\n",
        "# training_args 변수에 Seq2SeqTrainingArguments 객체를 생성합니다.\n",
        "# 각 인자는 다음과 같은 의미를 가집니다:\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    # output_dir: 모델 체크포인트와 기타 출력 파일이 저장될 디렉토리 경로\n",
        "    output_dir=\"fastwhisper\",  # change to a repo name of your choice\n",
        "\n",
        "    # per_device_train_batch_size: 각 GPU/CPU 장치당 훈련 배치 크기\n",
        "    per_device_train_batch_size=8,\n",
        "\n",
        "    # gradient_accumulation_steps: 경사도 누적 단계 수\n",
        "    # 이 값을 2배씩 늘리면 배치 크기를 1/2로 줄일 수 있습니다.\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "\n",
        "    # learning_rate: 학습률\n",
        "    learning_rate=1e-3,\n",
        "\n",
        "    # warmup_steps: 학습률 warmup 단계 수\n",
        "    warmup_steps=50,\n",
        "\n",
        "    # num_train_epochs: 훈련 에폭 수\n",
        "    num_train_epochs=1,\n",
        "\n",
        "    # evaluation_strategy: 평가 전략 (예: \"steps\", \"epoch\")\n",
        "    evaluation_strategy=\"steps\",\n",
        "\n",
        "    # fp16: 혼합 정밀도 사용 여부\n",
        "    fp16=True,\n",
        "\n",
        "    # per_device_eval_batch_size: 각 GPU/CPU 장치당 평가 배치 크기\n",
        "    per_device_eval_batch_size=8,\n",
        "\n",
        "    # generation_max_length: 생성 출력의 최대 길이\n",
        "    generation_max_length=128,\n",
        "\n",
        "    # logging_steps: 로깅 단계 수\n",
        "    logging_steps=100,\n",
        "\n",
        "    # max_steps: 최대 훈련 단계 수 (테스트 목적으로만 사용)\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "\n",
        "    # remove_unused_columns: 사용되지 않는 열 제거 여부\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "\n",
        "    # label_names: 레이블 이름 목록\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")\n",
        "\n",
        "\n",
        "# 이 코드는 Seq2SeqTrainingArguments 클래스를 사용하여 sequence-to-sequence 모델 학습을 위한 다양한 하이퍼파라미터를 정의하고 있습니다.\n",
        "# 이 클래스는 transformers 라이브러리에서 제공되며, 모델 학습 과정에서 필요한 여러 가지 설정을 할 수 있게 해줍니다.\n",
        "\n",
        "# 각 인자의 의미와 역할에 대해 자세히 설명했습니다.\n",
        "# 이를 통해 모델 학습 과정을 효과적으로 구성할 수 있습니다."
      ],
      "metadata": {
        "id": "8aKfy0lgEfVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT를 사용하여 모델을 미세 조정하려면 몇 가지 주의 사항이 있습니다.\n",
        "\n",
        "1. PeftModel의 전달은 기본 모델 전달의 서명을 상속하지 않으므로 `remove_unused_columns=False` 및 `label_names=[\"labels\"]`를 명시적으로 설정해야 합니다.\n",
        "\n",
        "2. INT8 훈련에는 자동 캐스팅이 필요하므로 Trainer에서 기본 `predict_with_generate` 호출은 자동으로 캐스팅되지 않으므로 사용할 수 없습니다.\n",
        "\n",
        "3. 마찬가지로 자동 캐스팅을 할 수 없기 때문에 `compute_metrics`를 `Seq2SeqTrainer`에 전달할 수 없으므로 Trainer를 인스턴스화하는 동안 주석 처리하겠습니다."
      ],
      "metadata": {
        "id": "q02qq6cmEiK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# 이 콜백은 어댑터 가중치만 저장하고 기본 모델 가중치는 제거합니다.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # 체크포인트 폴더 경로를 정의합니다.\n",
        "        # 체크포인트 폴더는 'checkpoint-{global_step}'로 이름이 지정됩니다.\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        # 어댑터 모델 가중치만 저장합니다.\n",
        "        # 이 작업은 PEFT 모델의 어댑터 가중치를 'adapter_model' 하위폴더에 저장합니다.\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        # 기본 모델 가중치 (pytorch_model.bin)를 제거하여 공간을 절약합니다.\n",
        "        # 어댑터 가중치만 저장하려고 하므로 이 파일은 필요하지 않습니다.\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "# 정의된 설정으로 Seq2SeqTrainer 인스턴스를 생성합니다.\n",
        "# 이 Trainer는 Seq2Seq 모델을 학습하는 데 사용됩니다.\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,  # 하이퍼파라미터를 포함한 학습 인자\n",
        "    model=model,  # 학습할 Seq2Seq 모델\n",
        "    train_dataset=common_voice[\"train\"],  # 학습 데이터셋\n",
        "    eval_dataset=common_voice[\"test\"],  # 평가 데이터셋\n",
        "    data_collator=data_collator,  # 학습 데이터 처리를 위한 데이터 콜레이터\n",
        "    tokenizer=processor.feature_extractor,  # 모델용 토크나이저\n",
        "    callbacks=[SavePeftModelCallback],  # PEFT 모델 가중치를 저장하기 위한 사용자 정의 콜백\n",
        ")\n",
        "\n",
        "# 모델 캐싱을 비활성화하여 경고를 억제합니다 (추론을 위해 활성화됨)\n",
        "# 이 설정은 학습에 권장되는 것이며 추론을 위해 활성화해야 합니다.\n",
        "model.config.use_cache = False\n",
        "\n",
        "\n",
        "\n",
        "# SavePeftModelCallback 클래스에 대한 설명: 이 콜백은 PEFT 모델의 어댑터 가중치만 저장하고 기본 모델 가중치는 제거합니다.\n",
        "# on_save 메서드에 대한 주석: 체크포인트 폴더 경로 정의, PEFT 모델의 어댑터 가중치 저장, 기본 모델 가중치 제거 등의 작업을 수행합니다.\n",
        "# Seq2SeqTrainer 객체 생성에 대한 주석: 모델, 데이터셋, 데이터 전처리기, 토크나이저 등의 인자를 전달하여 Seq2Seq 모델 학습을 위한 Trainer 객체를 생성합니다.\n",
        "# model.config.use_cache = False 설정에 대한 주석: 추론 시 사용하기 위해 캐싱 기능을 비활성화합니다."
      ],
      "metadata": {
        "id": "JilGQb5REjU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "FliM0wrQEkdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning한 모델을 Hugging Face Hub에 저장합니다. 나중에 모델을 불러올 때 편함"
      ],
      "metadata": {
        "id": "NfNDsavOEln2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"whisper-large-v2-korea-common_13\"\n",
        "model.push_to_hub(peft_model_id)"
      ],
      "metadata": {
        "id": "vxs1tDn9Emko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 평가 및 검증"
      ],
      "metadata": {
        "id": "Q3BLB_pfEn-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning을 성공적으로 했으면 이제 테스트 데이터셋에서 저희 모델을 테스트하고 CER(Character Error Rate) 계산해보겠습니다.\n",
        "\n",
        "테스트 유의할 점들:\n",
        "\n",
        "1. predict_with_generate 함수를 사용할 수 없으므로 자체적으로 torch.cuda.amp.autocast()를 사용하여 eval 루프를 직접 구현합니다.\n",
        "\n",
        "2. 기본 모델이 고정되어 있기 때문에 PEFT 모델은 때로 디코딩 중에 언어를 인식하지 못할 수 있습니다. 이를 해결하기 위해 디코딩 시작 토큰에 번역 중인 언어를 명시적으로 지정합니다. 이 작업은 forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")를 사용하여 수행하고 model.generate 호출에 이를 전달"
      ],
      "metadata": {
        "id": "6XFyI7eUEqub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# PEFT(Parameter-Efficient Fine-Tuning) 모델 및 구성 정보를 가져옵니다.\n",
        "peft_model_id = \"youngisk/whisper-large-v2-korea-common_13\"  # fine-tuned Whisper 모델의 ID\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)  # fine-tuned Whisper 모델의 구성 정보를 가져옵니다.\n",
        "\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "# Whisper 모델을 로드하고 PEFT 기술을 적용합니다.\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "# `WhisperForConditionalGeneration` 클래스를 사용하여 Whisper 모델을 로드합니다.\n",
        "# `load_in_8bit=True`는 모델을 8비트 정밀도로 로드하여 메모리 사용량을 줄입니다.\n",
        "# `device_map=\"auto\"`는 모델을 자동으로 여러 GPU에 분산하여 로드합니다.\n",
        "\n",
        "# PEFT 기술을 적용한 모델을 가져옵니다.\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "# `PeftModel` 클래스를 사용하여 fine-tuning된 Whisper 모델을 로드합니다.\n",
        "# 이를 통해 PEFT 기술을 적용할 수 있습니다.\n",
        "\n",
        "# 모델의 캐싱 기능을 활성화하여 추론 속도를 높입니다.\n",
        "model.config.use_cache = True\n",
        "\n",
        "\n",
        "\n",
        "# 이 코드는 PEFT 기술을 사용하여 fine-tuning된 Whisper 모델을 로드하고 구성하는 과정을 보여줍니다.\n",
        "# 모델을 8비트 정밀도로 로드하고 여러 GPU에 분산하여 메모리 사용량을 줄이며, PEFT 기술을 적용하여 모델의 성능을 향상시킵니다.\n",
        "# 마지막으로 모델의 캐싱 기능을 활성화하여 추론 속도를 높입니다."
      ],
      "metadata": {
        "id": "NM1ZlUBXEo90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # 가비지 컬렉션 모듈을 가져와 메모리 관리에 사용합니다.\n",
        "import numpy as np  # 배열 및 행렬 연산을 위한 NumPy 모듈을 가져옵니다.\n",
        "from tqdm import tqdm  # 진행 표시를 위한 tqdm 모듈을 가져옵니다.\n",
        "from torch.utils.data import DataLoader  # 데이터 로딩을 위한 PyTorch DataLoader 모듈을 가져옵니다.\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer  # 텍스트 정규화를 위한 모듈을 가져옵니다.\n",
        "\n",
        "# 평가 데이터 로더를 생성합니다.\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "# 강제 디코더 ID를 가져옵니다.\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "# 텍스트 정규화 객체를 생성합니다.\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "# 예측값, 참조값, 정규화된 예측값, 정규화된 참조값을 저장할 리스트를 초기화합니다.\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "# 모델을 평가 모드로 설정합니다.\n",
        "model.eval()\n",
        "# 평가 데이터로더를 반복하면서 추론을 수행합니다.\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():  # 자동 혼합 정밀도(AMP)를 사용하여 GPU 메모리를 효율적으로 활용합니다.\n",
        "        with torch.no_grad():  # 그라디언트 계산을 비활성화하여 메모리를 절약합니다.\n",
        "            # 모델을 사용하여 토큰을 생성합니다.\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),  # 입력 데이터를 GPU로 이동합니다.\n",
        "                    forced_decoder_ids=forced_decoder_ids,  # 강제 디코더 ID를 적용합니다.\n",
        "                    max_new_tokens=255,  # 최대 생성 토큰 수를 지정합니다.\n",
        "                )\n",
        "                .cpu()  # 생성된 토큰을 CPU로 이동합니다.\n",
        "                .numpy()  # 넘파이 배열로 변환합니다.\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()  # 레이블을 CPU로 이동하고 넘파이 배열로 변환합니다.\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)  # 레이블을 처리합니다.\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)  # 예측값을 디코딩합니다.\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)  # 레이블을 디코딩합니다.\n",
        "            predictions.extend(decoded_preds)  # 예측값을 리스트에 추가합니다.\n",
        "            references.extend(decoded_labels)  # 레이블을 리스트에 추가합니다.\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])  # 정규화된 예측값을 리스트에 추가합니다.\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])  # 정규화된 레이블을 리스트에 추가합니다.\n",
        "        del generated_tokens, labels, batch  # 불필요한 변수를 삭제하여 메모리를 해제합니다.\n",
        "    gc.collect()  # 메모리 해제를 위해 가비지 컬렉션을 수행합니다.\n",
        "\n",
        "# Word Error Rate(cer)를 계산합니다.\n",
        "cer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "# 정규화된 CER를 계산합니다.\n",
        "normalized_cer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "# 평가 메트릭을 저장합니다.\n",
        "eval_metrics = {\"eval/cer\": cer, \"eval/normalized_cer\": normalized_cer}\n",
        "\n",
        "# CER 및 정규화된 CER를 출력합니다.\n",
        "print(f\"{cer=} and {normalized_cer=}\")\n",
        "# 평가 메트릭을 출력합니다.\n",
        "print(eval_metrics)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 코드 설명\n",
        "# 평가 데이터 로더를 생성하고, 강제 디코더 ID와 텍스트 정규화 객체를 준비합니다.\n",
        "\n",
        "# 예측값, 참조값, 정규화된 예측값, 정규화된 참조값을 저장할 리스트를 초기화합니다.\n",
        "\n",
        "# 모델을 평가 모드로 설정하고, 평가 데이터로더를 반복하면서 추론을 수행합니다.\n",
        "# 자동 혼합 정밀도(AMP)를 사용하여 GPU 메모리를 효율적으로 활용합니다.\n",
        "# 그라디언트 계산을 비활성화하여 메모리를 절약합니다.\n",
        "# 모델을 사용하여 토큰을 생성하고, 예측값과 참조값을 디코딩하여 리스트에 저장합니다.\n",
        "# 정규화된 예측값과 참조값도 리스트에 저장합니다.\n",
        "# 불필요한 변수를 삭제하고 가비지 컬렉션을 수행하여 메모리를 해제합니다.\n",
        "\n",
        "# Word Error Rate(WER)와 정규화된 WER를 계산하고, 평가 메트릭을 저장합니다.\n",
        "\n",
        "# WER, 정규화된 WER, 평가 메트릭을 출력합니다.\n",
        "\n",
        "# 이 코드는 Whisper 모델의 성능을 평가하고 메트릭을 계산하는 과정을 보여줍니다.\n",
        "#  메모리 관리, 자동 혼합 정밀도 사용, 그라디언트 계산 비활성화 등의 기법을 통해 효율적인 추론 수행을 보여줍니다."
      ],
      "metadata": {
        "id": "YATdUbZdEs6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 마무리!\n"
      ],
      "metadata": {
        "id": "AkE0h_8iEvyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Whisper 체크포인트를 더 빠르고 저렴하게 훈련하고 CER에서 거의 손실이 없도록 학습하는 방법을 배움\n",
        "\n",
        "PEFT (Pretraining Efficiently with Fine-Tuning)를 사용하면 음성 인식 이외에도 다른 사전 훈련된 모델에 동일한 기술 세트를 적용 가능. 아래 링크에서 자세히 설명: https://github.com/huggingface/peft 🤗"
      ],
      "metadata": {
        "id": "gm4XLAOBEwyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper 모델 별 테스트"
      ],
      "metadata": {
        "id": "osOqju0IGOky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "mcU-Tbe-GSVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "yNlaD5K6GT4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 선택"
      ],
      "metadata": {
        "id": "Ee2ylD22GXQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "from transformers import WhisperProcessor\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# 일반 모델 로딩\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "# model_id = \"openai/whisper-base\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "\n",
        "# 노인발화데이터 3GB로 파인튜닝한 모델\n",
        "# \"openai/whisper-base\" fine_tuned moel\n",
        "# model = WhisperForConditionalGeneration.from_pretrained(\"/content/model\")\n",
        "# model.to(\"cuda\")\n",
        "# processor = WhisperProcessor.from_pretrained(\"/content/model\")"
      ],
      "metadata": {
        "id": "l7xASOJvGbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 허깅 페이스 로그인"
      ],
      "metadata": {
        "id": "m1wqVYa_GdAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- common_voice 14부터는 로그인 및 권한 확인 필요"
      ],
      "metadata": {
        "id": "R_icv5-TGebr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 : 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\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ED83Uf47GhhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## common_voice 로드 및 전처리"
      ],
      "metadata": {
        "id": "M9nspcFCGjKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = DatasetDict()\n",
        "test_version = \"mozilla-foundation/common_voice_15_0\"\n",
        "common_voice[\"test\"] = load_dataset(test_version, \"ko\", split=\"test\", use_auth_token=True)\n",
        "\n",
        "language_abbr = \"ko\"\n",
        "task = \"transcribe\"\n",
        "\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "5feeqZhWGkJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "feature_extractor=processor.feature_extractor\n",
        "tokenizer=processor.tokenizer\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"test\"], num_proc=1)"
      ],
      "metadata": {
        "id": "Aet84e2tGlb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가진행"
      ],
      "metadata": {
        "id": "xuPGoKB-Gngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- test : common_voice15"
      ],
      "metadata": {
        "id": "ylMRdDtCGofp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_abbr, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "cer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_cer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/cer\": cer, \"eval/normalized_cer\": normalized_cer}\n",
        "\n",
        "print(f\"{cer=}  and {normalized_cer=}  \")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "U5y-CPIoGpFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 이용해서 오디오파일 전사"
      ],
      "metadata": {
        "id": "vBq1NGa1Gq5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이프라인 설정\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=30,\n",
        "    batch_size=16,\n",
        "    return_timestamps=True,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "-LkIUnahGrn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 전 common_voice 15 오디오 파일 로딩\n",
        "test = DatasetDict()\n",
        "test_version = \"mozilla-foundation/common_voice_15_0\"\n",
        "test[\"test\"] = load_dataset(test_version, \"ko\", split=\"test\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "aVZBZz2uGsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노인발화데이터 테스트(10개)\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "answer = [\n",
        "\"저 사람이 나한테 저렇게 행동을 하는구나\",\n",
        "\"그러고 내가 더 그 사람한테 저 사람이 모자란 게 무엇이고\",\n",
        "\"저 사람이 원하는 게 무엇인가를 내가 생각해봐야지\",\n",
        "\"그렇게 사람한테 베풀어주면 싸울일이 없다고 생각해\",\n",
        "\"아무리 극에 달해서 화가 났어도 내가 좀 참고\",\n",
        "\"옛날에 장사를 했으니까 뭐\",\n",
        "\"그런데 앞으로는 상가도 조금 조심해서 해야 되겠더라고 보니까\",\n",
        "\"그러니까 요즘에는 온라인이 너무 그렇지\",\n",
        "\"뭐 온라인 쇼핑이 너무 성대 하니까\",\n",
        "\"오프라인이 처지지 어쩔 수 없는거야\"]\n",
        "\n",
        "for file, an in zip(sorted(glob(\"/content/노인발화TEST/*\")), answer):\n",
        "  print(pipe(file)[\"text\"])\n",
        "  print(an)\n"
      ],
      "metadata": {
        "id": "M1adPKv4GtRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이프라인에 있는 모델로 common_voice15 test 오디오파일 전사(10개)\n",
        "for i in range(10):\n",
        "  result = pipe(test[\"test\"][\"audio\"][i][\"path\"])\n",
        "  print(result[\"text\"])\n",
        "  print(test[\"test\"][\"sentence\"][i])"
      ],
      "metadata": {
        "id": "j39YY2Z8GuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faster_whisper"
      ],
      "metadata": {
        "id": "__l1sRXoGvWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast Whisper로 테스트"
      ],
      "metadata": {
        "id": "ltew6NKLGxSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast Whisper 기본 버전 테스트\n",
        "from faster_whisper import WhisperModel\n",
        "from faster_whisper.feature_extractor import FeatureExtractor\n",
        "from faster_whisper.tokenizer import Tokenizer\n",
        "import tokenizers\n",
        "\n",
        "model = WhisperModel(\"large-v3\")\n",
        "feature_extractor = FeatureExtractor()\n",
        "tokenizer = Tokenizer(tokenizers.Tokenizer, False)"
      ],
      "metadata": {
        "id": "CwvROtIhGys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노인발화데이터 10개 테스트\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "answer = [\n",
        "\"저 사람이 나한테 저렇게 행동을 하는구나\",\n",
        "\"그러고 내가 더 그 사람한테 저 사람이 모자란 게 무엇이고\",\n",
        "\"저 사람이 원하는 게 무엇인가를 내가 생각해봐야지\",\n",
        "\"그렇게 사람한테 베풀어주면 싸울일이 없다고 생각해\",\n",
        "\"아무리 극에 달해서 화가 났어도 내가 좀 참고\",\n",
        "\"옛날에 장사를 했으니까 뭐\",\n",
        "\"그런데 앞으로는 상가도 조금 조심해서 해야 되겠더라고 보니까\",\n",
        "\"그러니까 요즘에는 온라인이 너무 그렇지\",\n",
        "\"뭐 온라인 쇼핑이 너무 성대 하니까\",\n",
        "\"오프라인이 처지지 어쩔 수 없는거야\"]\n",
        "\n",
        "for file, an in zip(sorted(glob(\"/content/노인발화TEST/*\")), answer):\n",
        "  segments, info = model.transcribe(file)\n",
        "  for segment in segments:\n",
        "    print(segment.text)\n",
        "  print(an)"
      ],
      "metadata": {
        "id": "ulmExcLGGzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이프라인에 있는 모델로 common_voice15 test 오디오파일 전사(10개)\n",
        "for i in range(10):\n",
        "  segments, info = model.transcribe(test[\"test\"][\"audio\"][i][\"path\"])\n",
        "  for segment in segments:\n",
        "    print(segment.text)\n",
        "  print(test[\"test\"][\"sentence\"][i])"
      ],
      "metadata": {
        "id": "R_KNzBdbG0Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다른 테스트셋으로 추가 평가\n"
      ],
      "metadata": {
        "id": "45igwURdG1tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- test : 노인발화[\"test\"]로 추가 테스트"
      ],
      "metadata": {
        "id": "fRo5q0ReG22E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "xj4U8q70G3mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노인발화 전처리된 testset 불러오기\n",
        "#로드\n",
        "from datasets import load_from_disk\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "noin_test = load_from_disk(\"/content/test\")\n",
        "noin_test"
      ],
      "metadata": {
        "id": "rKa-551bG4JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노인발화[\"test\"]\n",
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "eval_dataloader = DataLoader(noin_test, batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_abbr, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "cer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_cer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/cer\": cer, \"eval/normalized_cer\": normalized_cer}\n",
        "\n",
        "print(f\"{cer=}  and {normalized_cer=}  \")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "yC8_eD5CG46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 서비스 구현 (시각화 전 Ver)"
      ],
      "metadata": {
        "id": "pPQE8GnsIjLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast-Wshisper 모델 로딩"
      ],
      "metadata": {
        "id": "_A5muwFrIoJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import time\n",
        "import pygame\n",
        "import os\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from faster_whisper import WhisperModel\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1kUxW-fQIlND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인텔 드라이버 충돌 에러\n",
        "# 원래 있는데 다시 깔려고 하니까 중복 에러 뜸\n",
        "# 중복돼도 OK에 True값 적용\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "# Fast Whisper 모델 불러오기\n",
        "model = WhisperModel(\"large-v3\")"
      ],
      "metadata": {
        "id": "uVe2UapTIrfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 질문지 작성"
      ],
      "metadata": {
        "id": "41O1PkwPIrIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_ment = \"\"\"안녕하세요. 호석님. 서울시 중구 취업 센터 aI 상담사입니다.\n",
        "시니어 취업 관련해서 간단한 사전 설문조사를 진행하고 있사오니, 잠깐 시간을 내어주시면 감사하겠습니다.\"\"\"\n",
        "\n",
        "question1 = \"먼저 성함을 알려주시겠어요?\"\n",
        "\n",
        "question2 = \"성별과 나이는 어떻게 되시나요?\"\n",
        "\n",
        "question3 = \"\"\"건강상태를 간단히 여쭤보겠습니다.\n",
        "걷는데는 지장이 없으신가요? 있으시다면 얼마만큼 불편하신가요?\"\"\"\n",
        "\n",
        "question4 = \"보고 듣는데는 지장이 없으신가요? 있으시다면 얼마만큼 불편하신가요?\"\n",
        "\n",
        "question5 = \"\"\"학교는 어디까지 나오셨나요?\n",
        "초등학교, 중학교, 고등학교, 대학교에 졸업한 곳으로 답변 부탁드립니다.\"\"\"\n",
        "\n",
        "question6 = \"살면서 주로 무슨 일들을 하셨나요?\"\n",
        "\n",
        "question7 = \"혹시 특별히 하고 싶은 일이 있으신가요?\"\n",
        "\n",
        "question8 = \"배우자나 자녀분들은 어디에 살고 있나요?\"\n",
        "\n",
        "end_ment = \"\"\"답변 감사드립니다. 서울시 중구 고용센터였습니다.\n",
        "궁금하신 사항이 있으면 발송드린 문자에 있는 번호로 연락 부탁드립니다.\n",
        "담당자 검토 후 수일 내로 다시 연락드리겠습니다. 감사합니다. \"\"\"\n",
        "\n",
        "# 질문 갯수\n",
        "questions = [start_ment, question1, question2, question3, question4, question5, question6, question7, question8, end_ment]\n",
        "\n",
        "# 답변 담을 리스트\n",
        "answer = []"
      ],
      "metadata": {
        "id": "0Om5B0k1It7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 메소드 작성"
      ],
      "metadata": {
        "id": "5F4YRHtBItjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 텍스트를 말로 바꿔줌\n",
        "# param : 읽어줄 텍스트\n",
        "def speak(text):\n",
        "    tts = gTTS(text=text, lang='ko')\n",
        "    filename='voice.mp3'\n",
        "    tts.save(filename)\n",
        "    pygame.init()\n",
        "    pygame.mixer.music.load(filename)\n",
        "    pygame.mixer.music.play()\n",
        "    while pygame.mixer.music.get_busy():\n",
        "        continue\n",
        "    pygame.mixer.music.unload()\n",
        "    os.remove(filename)\n",
        "\n",
        "\n",
        "# 마이크 음성 -> 오디오 파일로 저장 (구글이 만들어준 텍스트파일도 같이 저장)\n",
        "# pram : 저장할 폴더 경로(str), 사용자정보(list), 질문번호(int)\n",
        "def get_audio(folder_path, user_info, i):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "\n",
        "        print(\"지금 말씀하세요: \")\n",
        "\n",
        "        # 오디오 파일로 저장\n",
        "        audio = r.listen(source)\n",
        "        wav_file_name = folder_path + '\\\\' + user_info[0] + \"_\" + user_info[1] + \"_\" + str(i)\n",
        "\n",
        "        # 중복된거 있으면 그냥 지우고 다시 생성\n",
        "        if os.path.isfile(wav_file_name + \".wav\"):\n",
        "            os.remove(wav_file_name + \".wav\")\n",
        "\n",
        "        try:\n",
        "            with open(wav_file_name + \".wav\", 'bx') as f:\n",
        "                f.write(audio.get_wav_data())\n",
        "        except Exception as e:\n",
        "            print(\"Exception: \" + str(e))\n",
        "\n",
        "        # 구글에서 전사해준거 텍스트로 저장 (위스퍼랑 비교해보자)\n",
        "        text = \"\"\n",
        "        try:\n",
        "            txt_file_name = folder_path + '\\\\' + user_info[0] + \"_\" + user_info[1] + \"_google.txt\"\n",
        "            text = r.recognize_google(audio, language=\"ko-KR\")\n",
        "            print(\"말씀하신 내용입니다 : \", text)\n",
        "\n",
        "            # 답변 전사된 텍스트 파일로 만들어 저장하기\n",
        "            with open(txt_file_name, 'a') as f:\n",
        "                f.write(str(text)+\"\\n\")\n",
        "        except Exception as e:\n",
        "            print(\"Exception: \" + str(e))\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# #### 폴더 만들기 ####\n",
        "# param : 만들 폴더 주소(str)\n",
        "def make_folder(path: str):\n",
        "\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "\n",
        "    # 혹시 이름 중복이면 일단 다른 폴더로 만들기\n",
        "    # 해당 내용 로그 파일에 기록\n",
        "    except:\n",
        "        os.mkdir(path + \"_Error\")\n",
        "        print(f\"중복 폴더 존재{path}\")\n",
        "\n",
        "\n",
        "#### 녹음별 유저 폴더 만들기 ####\n",
        "# param : 유저 정보 리스트(list)\n",
        "# return : 만들어진 폴더 경로(str)\n",
        "def make_user_voice_folder(user_info: list):\n",
        "\n",
        "    try :\n",
        "        date = datetime.today().strftime(\"%Y%m%d\")\n",
        "        num = user_info[1]\n",
        "        name = user_info[0]\n",
        "\n",
        "        # 상위 폴더는 일단 날짜명. 없으면 만들어줌\n",
        "        if os.path.exists(f\"03.음성녹음\\\\{date}\") == False:\n",
        "            os.mkdir(\"03.음성녹음\\\\\" + date)\n",
        "\n",
        "        # 폴더가 없다면\n",
        "        folder_path = \"03.음성녹음\\\\\" + date + \"\\\\\" + num + \"_\" + name\n",
        "        if os.path.exists(folder_path) == False:\n",
        "\n",
        "            # 날짜 폴더 밑에 [식별번호_이름] 폴더를 만들어줌\n",
        "            make_folder(folder_path)\n",
        "\n",
        "    # 혹시 중복일 경우 일단 다른 폴더를 만들어주고 로그 남김\n",
        "    except Exception as e:\n",
        "        print(\"Exception: \" + str(e))\n",
        "\n",
        "    return folder_path\n",
        "\n",
        "# 위스퍼로 오디오파일 전사하기\n",
        "# param : 폴더경로(str)\n",
        "# return : 답변 텍스트 리스트(list)\n",
        "def whisper_transcribe(folder_path):\n",
        "\n",
        "    answer_lst = []\n",
        "    for audio_file in sorted(glob(folder_path + \"/*.wav\")):\n",
        "        segments, info = model.transcribe(audio_file)\n",
        "        for segment in segments:\n",
        "            answer_lst.append(segment.text)\n",
        "\n",
        "    return answer_lst\n",
        "\n",
        "\n",
        "# GPT야 요약해줘\n",
        "# param : 답변 내용 리스트(list)\n",
        "# return : 요약내용 리스트(list)\n",
        "def get_gpt_help(answer_lst):\n",
        "    # 재우 API\n",
        "    GPT_API_KEY = \"sk-proj-KgZChuKTiML1O6WHDO7oT3BlbkFJvJYPMkdQyzZ3v4cc9XML\"\n",
        "\n",
        "    # API 키로 LLM 객체 생성 (GPT와 연결해줌)\n",
        "    # temperature : 생성된 텍스트의 다양성 조정\n",
        "    # 0~2 사이\n",
        "    # 높을 수록 출력을 무작위하게, 낮을 수록 출력을 더 집중되고 결정론적으로 만듦\n",
        "    # model_name : 사용할 GPT 모델(버전) 정보\n",
        "    # openai_api_key : API 키값\n",
        "    chat = ChatOpenAI(temperature = 0.2, max_tokens = 2024, openai_api_key = GPT_API_KEY)\n",
        "\n",
        "    system_msg = \"이 글에서 '이c름, 나이, 건강상태, 최종학력, 경력사항, 희망사항, 가족사항' 으로 요약해줘\"\n",
        "    human_msg = \".\".join(answer_lst)\n",
        "\n",
        "\n",
        "    # 시스템 메세지로 원하는 답변의 형태를 지정할 수 있음\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=human_msg),\n",
        "    ]\n",
        "\n",
        "    # 질문하고 답받아서 잘라서 리스트로 만들기\n",
        "    gpt_re_lst = str(chat.invoke(messages).content).split('\\n')\n",
        "\n",
        "    return gpt_re_lst\n",
        "\n"
      ],
      "metadata": {
        "id": "w927S5MAIxyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 서비스 코드 작성"
      ],
      "metadata": {
        "id": "IgRFKEeAIzXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 유저 폴더 만들기 및 경로 추출\n",
        "user_info = [\"001234001213\", \"김옥자\", \"\", \"\"]\n",
        "folder_path = make_user_voice_folder(user_info)\n",
        "\n",
        "\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "\n",
        "    # 질문하기\n",
        "    speak(question)\n",
        "\n",
        "    # 첫 번째랑 마지막은 안내멘트로 답변 안받음\n",
        "    if 0 < i < len(questions) - 1:\n",
        "\n",
        "        # 답변받고 파일 저장(오디오파일, 텍스트파일)\n",
        "        get_audio(folder_path, user_info, i)\n",
        "\n",
        "# 혹시 테스트하다가 중간에 그냥 끊으면 플레이어 언로드해주기\n",
        "pygame.mixer.music.unload\n",
        "\n",
        "# 위스퍼로 해당 폴더 오디오 내용 전사하기\n",
        "answer_lst = whisper_transcribe(folder_path)\n",
        "\n",
        "# 랭체인 GPT로 요약해서 받기\n",
        "gpt_re_lst = get_gpt_help(answer_lst)\n",
        "\n",
        "\n",
        "# 데이터프레임으로 만들기\n",
        "\n",
        "# 답변 내용만 가져와서 리스트로 만들기\n",
        "# 검증 코드가 필요하겠지만.. 여기까지만...\n",
        "# 실제로 사용한다면 DB에다가 저장\n",
        "content = []\n",
        "for sentence in gpt_re_lst:\n",
        "    content.append(sentence.split(\": \")[1])\n",
        "\n",
        "result_df = pd.DataFrame(columns =[\"이름\",\"나이\",\"건강상태\",\"최종학력\",\"경력사항\",\"희망사항\",\"가족사항\"])\n",
        "result_df.loc[len(result_df)] = content\n"
      ],
      "metadata": {
        "id": "miYpCJpiI0Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 저장 데이터프레임\n",
        "result_df"
      ],
      "metadata": {
        "id": "Qr5tgIJEI3C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 엑셀로 추출\n",
        "result_df.to_excel(\"조사결과.xlsx\", index = False)"
      ],
      "metadata": {
        "id": "StR-_p17I3tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 서비스 구현 (웹 시각화 Ver)"
      ],
      "metadata": {
        "id": "nuJG3fornwds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코랩 실행 안됩니다.\n",
        "로컬에서 실행해 주세요."
      ],
      "metadata": {
        "id": "uW5lVLRVn1_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from audio_recorder_streamlit import audio_recorder\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from faster_whisper import WhisperModel\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "\n",
        "# 마이크 음성 -> 오디오 파일로 저장 (구글이 만들어준 텍스트파일도 같이 저장)\n",
        "# pram : 저장할 폴더 경로(str), 사용자정보(list), 질문번호(int)\n",
        "def get_audio(folder_path, user_info, i):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "\n",
        "        print(\"지금 말씀하세요: \")\n",
        "\n",
        "        # 오디오 파일로 저장\n",
        "        audio = r.listen(source)\n",
        "        wav_file_name = folder_path + '\\\\' + user_info[0] + \"_\" + user_info[1] + \"_\" + str(i)\n",
        "\n",
        "        # 중복된거 있으면 그냥 지우고 다시 생성\n",
        "        if os.path.isfile(wav_file_name + \".wav\"):\n",
        "            os.remove(wav_file_name + \".wav\")\n",
        "\n",
        "        try:\n",
        "            with open(wav_file_name + \".wav\", 'bx') as f:\n",
        "                f.write(audio.get_wav_data())\n",
        "        except Exception as e:\n",
        "            print(\"Exception: \" + str(e))\n",
        "\n",
        "        # 구글에서 전사해준거 텍스트로 저장 (위스퍼랑 비교해보자)\n",
        "        text = \"\"\n",
        "        try:\n",
        "            txt_file_name = folder_path + '\\\\' + user_info[0] + \"_\" + user_info[1] + \"_google.txt\"\n",
        "            text = r.recognize_google(audio, language=\"ko-KR\")\n",
        "            print(\"말씀하신 내용입니다 : \", text)\n",
        "\n",
        "            # 답변 전사된 텍스트 파일로 만들어 저장하기\n",
        "            with open(txt_file_name, 'a') as f:\n",
        "                f.write(str(text)+\"\\n\")\n",
        "        except Exception as e:\n",
        "            print(\"Exception: \" + str(e))\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# #### 폴더 만들기 ####\n",
        "# param : 만들 폴더 주소(str)\n",
        "def make_folder(path: str):\n",
        "\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "\n",
        "    # 혹시 이름 중복이면 일단 다른 폴더로 만들기\n",
        "    # 해당 내용 로그 파일에 기록\n",
        "    except:\n",
        "        os.mkdir(path + \"_Error\")\n",
        "        print(f\"중복 폴더 존재{path}\")\n",
        "\n",
        "\n",
        "#### 녹음별 유저 폴더 만들기 ####\n",
        "# param : 유저 정보 리스트(list)\n",
        "# return : 만들어진 폴더 경로(str)\n",
        "def make_user_voice_folder(user_info: list):\n",
        "\n",
        "    try :\n",
        "        date = datetime.today().strftime(\"%Y%m%d\")\n",
        "        num = user_info[1]\n",
        "        name = user_info[0]\n",
        "\n",
        "        # 상위 폴더는 일단 날짜명. 없으면 만들어줌\n",
        "        if os.path.exists(f\"03.음성녹음\\\\{date}\") == False:\n",
        "            os.mkdir(\"03.음성녹음\\\\\" + date)\n",
        "\n",
        "        # 폴더가 없다면\n",
        "        folder_path = \"03.음성녹음\\\\\" + date + \"\\\\\" + num + \"_\" + name\n",
        "        if os.path.exists(folder_path) == False:\n",
        "\n",
        "            # 날짜 폴더 밑에 [식별번호_이름] 폴더를 만들어줌\n",
        "            make_folder(folder_path)\n",
        "\n",
        "    # 혹시 중복일 경우 일단 다른 폴더를 만들어주고 로그 남김\n",
        "    except Exception as e:\n",
        "        print(\"Exception: \" + str(e))\n",
        "\n",
        "    return folder_path\n",
        "\n",
        "# 위스퍼로 오디오파일 전사하기\n",
        "# param : 폴더경로(str)\n",
        "# return : 답변 텍스트 리스트(list)\n",
        "def whisper_transcribe(folder_path, model):\n",
        "\n",
        "    answer_lst = []\n",
        "    for audio_file in sorted(glob(folder_path + \"\\\\*.wav\")):\n",
        "        segments, info = model.transcribe(audio_file)\n",
        "        for segment in segments:\n",
        "            answer_lst.append(segment.text)\n",
        "\n",
        "    return answer_lst\n",
        "\n",
        "\n",
        "# GPT야 요약해줘\n",
        "# param : 답변 내용 리스트(list)\n",
        "# return : 요약내용 리스트(list)\n",
        "def get_gpt_help(answer_lst):\n",
        "    # 재우 API\n",
        "    GPT_API_KEY = \"sk-proj-KgZChuKTiML1O6WHDO7oT3BlbkFJvJYPMkdQyzZ3v4cc9XML\"\n",
        "\n",
        "    # API 키로 LLM 객체 생성 (GPT와 연결해줌)\n",
        "    # temperature : 생성된 텍스트의 다양성 조정\n",
        "    # 0~2 사이\n",
        "    # 높을 수록 출력을 무작위하게, 낮을 수록 출력을 더 집중되고 결정론적으로 만듦\n",
        "    # model_name : 사용할 GPT 모델(버전) 정보\n",
        "    # openai_api_key : API 키값\n",
        "    chat = ChatOpenAI(temperature = 0.2, max_tokens = 2024, openai_api_key = GPT_API_KEY)\n",
        "\n",
        "    system_msg = \"이 글에서 '이c름, 나이, 건강상태, 최종학력, 경력사항, 희망사항, 가족사항' 으로 요약해줘\"\n",
        "    human_msg = \".\".join(answer_lst)\n",
        "\n",
        "\n",
        "    # 시스템 메세지로 원하는 답변의 형태를 지정할 수 있음\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=human_msg),\n",
        "    ]\n",
        "\n",
        "    # 질문하고 답받아서 잘라서 리스트로 만들기\n",
        "    gpt_re_lst = str(chat.invoke(messages).content).split('\\n')\n",
        "\n",
        "    return gpt_re_lst\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################### 시각화 - 질문&답변 #########################\n",
        "\n",
        "# streamlit run() 스톱 잘되게 하는 코드\n",
        "# if st.button('Stop the app'): # 버튼 누르면\n",
        "#     st.session_state['stop'] = True\n",
        "\n",
        "if 'stop' not in st.session_state: # Cont + C 누르면\n",
        "    st.session_state['stop'] = False\n",
        "\n",
        "if st.session_state['stop']: # 브라우저 꺼지면\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "\n",
        "# 메인화면\n",
        "st.title('4조(FORS)')\n",
        "st.header('Whisper Fine Thank you & U?')\n",
        "st.title('')\n",
        "st.title('')\n",
        "\n",
        "# 질문 파일이름모음 및 iter 객체 생성\n",
        "try:\n",
        "\n",
        "    # 질문 파일이름 iter 객체 생성\n",
        "    if \"questions\" not in st.session_state:\n",
        "        st.session_state.questions = [\"03.음성녹음\\\\Streamlit\\\\Question\\\\question\" + str(i) + \".wav\" for i in range(10)]\n",
        "        st.session_state.iter_question = iter(st.session_state.questions)\n",
        "\n",
        "        # 첫 질문 초기화\n",
        "        st.session_state.filename = next(st.session_state.iter_question)\n",
        "\n",
        "        # 첫 질문넘버 초기화\n",
        "        st.session_state.question = \"시작 안내 멘트\"\n",
        "        st.session_state.question_num = 0\n",
        "\n",
        "        # 폴더 생성 여부 체크\n",
        "        st.session_state.isfolder = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 버튼 누르면 다음 질문으로 넘어가도록 함\n",
        "    if st.button(\"다음질문\"):\n",
        "\n",
        "        # 다음질문으로 파일이름 변경\n",
        "        st.session_state.filename = next(st.session_state.iter_question)\n",
        "\n",
        "        # 질문 번호 변경\n",
        "        st.session_state.question_num += 1\n",
        "\n",
        "        # 마지만 안내멘트 설정\n",
        "        if st.session_state.question_num == len(st.session_state.questions) - 1:\n",
        "            st.session_state.question = \"마지막 안내 멘트\"\n",
        "\n",
        "        elif st.session_state.question_num == 0:\n",
        "            st.session_state.question = \"시작 안내 멘트\"\n",
        "\n",
        "        # 질문번호 설정\n",
        "        else:\n",
        "            st.session_state.question = \"질문 \" + str(st.session_state.question_num)\n",
        "\n",
        "    # 질문 번호 출력\n",
        "    st.subheader(st.session_state.question)\n",
        "\n",
        "    # 오디오 출력\n",
        "    st.audio(st.session_state.filename, format=\"audio/wav\")\n",
        "\n",
        "    # 녹음기삽입\n",
        "    bytes = audio_recorder(sample_rate = 16000, energy_threshold = 1.0, pause_threshold = 2, key = st.session_state.question_num)\n",
        "\n",
        "\n",
        "    if bytes:\n",
        "        user_info = [\"001234001213\", \"김두한\", \"\", \"\"]\n",
        "\n",
        "        # 폴더 없으면 만들어주기\n",
        "        if st.session_state.isfolder == False:\n",
        "            st.session_state.folder_path = make_user_voice_folder(user_info)\n",
        "            st.session_state.isfolder = True\n",
        "\n",
        "        # 저장할 파일 이름\n",
        "        wav_file_name = st.session_state.folder_path + '\\\\' + user_info[0] + \"_\" + user_info[1] + \"_\" + str(st.session_state.question_num)\n",
        "\n",
        "        # 오디오 파일로 저장\n",
        "        try:\n",
        "            with open(wav_file_name + \".wav\", 'bx') as f:\n",
        "                f.write(bytes)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Exception: \" + str(e))\n",
        "\n",
        "\n",
        "# next 객체 마지막에 도달했을 경우\n",
        "except Exception as e:\n",
        "    st.text(\"설문조사가 모두 끝났습니다.\")\n",
        "    print(e)\n",
        "\n",
        "\n",
        "st.title('')\n",
        "st.title('')\n",
        "\n",
        "\n",
        "####################### 시각화 - 위스퍼 전사 ########################\n",
        "\n",
        "if st.button(\"위스퍼 텍스트 출력\"):\n",
        "\n",
        "    # 위스퍼 모델 초기화\n",
        "    if \"model\" not in st.session_state:\n",
        "\n",
        "        # 인텔 드라이버 충돌 에러\n",
        "        # 원래 있는데 다시 깔려고 하니까 중복 에러 뜸\n",
        "        # 중복돼도 OK에 True값 적용\n",
        "\n",
        "        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "        # Fast Whisper 모델 불러오기\n",
        "        st.session_state.model = WhisperModel(\"large-v3\")\n",
        "\n",
        "    st.session_state.answer_lst = whisper_transcribe(st.session_state.folder_path, st.session_state.model)\n",
        "    for text in st.session_state.answer_lst:\n",
        "        st.text(text)\n",
        "\n",
        "\n",
        "####################### 시각화 - GPT 요약 ########################\n",
        "\n",
        "# st.session_state.answer_lst = [\n",
        "#     \"내 이름이 음 김옥순이여어@#!#\",\n",
        "#     \"내는 남자제. 나이는 뭐 육십일곱인기라\"\n",
        "#     \"나 뭐 걸을 수는 있는데 뭐냐 그 가끔은 무릎이 시려서 좀 어려울 때가 있어~\",\n",
        "#     \"눈이야 좀 침침허지이 귀도 잘 안들려 그래도 사는데 지장은 없어~\",\n",
        "#     \"어.. 그.. 내는 중학교뿌이 안나왔다아이가. 고등학교는 못갔데이\",\n",
        "#     \"뭐.. 예전에는 과일도 좀 팔고.. 생선도 팔고.. 팔 수 있는거는 다 팔았다 아이가. 직접 운전해서 배달도 하고.. \",\n",
        "#     \"나야 뭐 아무일이나 하면 좋지.. 사람들이랑 좀 얘기도 하고 몸도 좀 움직이고 하는 일이었으면 좋겠는데..\",\n",
        "#     \"남편은 집에서 같이 살고 있쟤. 딸래미는 시집가서 서울에서 살고 있고 아들래미는 학교 댕긴다고 저기 부산에 가 있다 아이가\",\n",
        "# ]\n",
        "\n",
        "\n",
        "if st.button(\"GPT 요약\"):\n",
        "    st.session_state.gpt_re_lst = get_gpt_help(st.session_state.answer_lst)\n",
        "\n",
        "    for text in st.session_state.gpt_re_lst:\n",
        "        st.text(text)\n",
        "\n",
        "\n",
        "####################### 시각화 - 데이터프레임 ########################\n",
        "\n",
        "if st.button(\"데이터프레임\"):\n",
        "    content = [sentence.split(\": \")[1] for sentence in st.session_state.gpt_re_lst]\n",
        "    st.session_state.result_df = pd.DataFrame(columns =[\"이름\",\"나이\",\"건강상태\",\"최종학력\",\"경력사항\",\"희망사항\",\"가족사항\"])\n",
        "    st.session_state.result_df.loc[len(st.session_state.result_df)] = content\n",
        "    st.dataframe(st.session_state.result_df)\n",
        "\n",
        "####################### 시각화 - 엑셀 추출 ########################\n",
        "\n",
        "if st.button(\"엑셀 파일 추출\"):\n",
        "    st.session_state.result_df.to_excel(\"조사결과.xlsx\", index = False)"
      ],
      "metadata": {
        "id": "h0idJoiKnzX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}