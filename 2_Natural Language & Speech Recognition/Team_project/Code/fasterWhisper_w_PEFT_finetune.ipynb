{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaterPurify/Alpaco_Bootcamp/blob/main/2_Natural%20Language%20%26%20Speech%20Recognition/Team_project/Code/fasterWhisper_w_PEFT_finetune_version1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cefac89",
      "metadata": {
        "id": "5cefac89"
      },
      "source": [
        "# LoRA (Low Rank Adaptation) 와 PEFT (Paramater Efficient Fine Tuning)를 적용한 Larger Whisper fine-tuning ⚡️\n",
        "\n",
        "* 소비자 GPU의 VRAM이 8GB 미만인 환경에서도 full-finetuning과 유사한 성능을 제공함\n",
        "*  🤗 Transformers and PEFT 모델과 Common Voice 13.0 dataset를 사용하여 Whisper fine-tune하는 과정 설명함\n",
        "* PEFT와 bitsandbytes를 활용하여 무료 T4 GPU(16GB VRAM)를 사용하여 whisper-large-v2 체크포인트를 원활하게 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i7DzwD7oYDPf",
      "metadata": {
        "id": "i7DzwD7oYDPf"
      },
      "source": [
        "## 왜 Parameter Efficient Fine Tuning [PEFT](https://github.com/huggingface/peft)를 사용해야 되는가?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZAMw8FIuYMF-",
      "metadata": {
        "id": "ZAMw8FIuYMF-"
      },
      "source": [
        "* 모델 사이즈 증가로 fine tuning하는 것이 계산 복잡성 증가와 메모리 사용량 증가\n",
        "    * 예를 들어, Whisper-large-v2 모델을 완전한 미세 조정을 위해 약 24GB의 GPU VRAM이 필요하며, 각 미세 조정된 모델은 약 7GB의 저장 공간을 필요함\n",
        "\n",
        "    * 제한적인 환경에서 bottleneck 발생하고 원하는 결과를 얻기 힘듦\n",
        "* PEFT\n",
        "    * 효과적으로 parameter를 줄여서 fine tuning 속도 개선\n",
        "    * 목적: 병목 현상을 해결\n",
        "    * 접근법(예: 저수준 적응): 사전 훈련된 모델의 대부분의 매개변수를 동결시키면서 추가 모델 매개변수의 일부만 미세 조정하여 계산 및 저장 비용 크게 줄임\n",
        "        * 대규모 모델의 전체 미세 조정 중 관찰되는 catastrophic forgetting 문제를 극복할 수 있음\n",
        "\n",
        "### LoRA가 무엇인가?\n",
        "\n",
        "* PEFT에서 여러 매개변수 효율적인 기술을 기본으로 제공\n",
        "    * 그 중 하나인 Low Rank Adaptation (LoRA)\n",
        "        * 사전 훈련된 모델 가중치를 동결하고 Transformer 아키텍처의 각 레이어에 훈련 가능한 랭크 분해 행렬을 삽입 (High Rank 즉 많은 연결이 되어 있는 것들보다 연결이 적은 Low Rank로 만들어서 계산량을 줄임)\n",
        "            * Downstream 작업에 대한 훈련 가능한 매개변수 수가 크게 감소\n",
        "\n",
        "### 통계로 보는 PEFT 효과\n",
        "\n",
        "* Full fine-tuning of Whisper-large-v2 checkpoint Vs. PEFT 적용 모델\n",
        "\n",
        "    1. GPU VRAM이 8GB 미만인 환경에서 16억 개의 매개변수를 가진 모델을 미세 조정 🤯\n",
        "    2. 훨씬 적은 수의 훈련 가능한 매개변수를 사용하여 거의 5배 더 큰 배치 크기를 사용 가능 📈\n",
        "    3. 생성된 체크포인트는 원본 모델의 크기의 1%인 약 60MB 🚀\n",
        "* 기존 🤗 transformers Whisper에서 변형이 많이 되지 않았음\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625e47a0",
      "metadata": {
        "id": "625e47a0"
      },
      "source": [
        "## 환경설정\n",
        "\n",
        "\n",
        "* Python package->Whisper 모델 fine tuning하기 위해 사용\n",
        "  * `datasets`:학습 데이터를 다운로드하고 준비\n",
        "  * `transformers`: Whisper 모델을 로드하고 훈련\n",
        "  * `librosa`: 오디오 파일을 전처리\n",
        "  * `evaluate` &  `jiwer`:모델의 성능을 평가\n",
        "  * `PEFT`, `bitsandbytes`, `accelerate`: LoRA로 모델과 fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r_Ivl7qlX0dz",
      "metadata": {
        "id": "r_Ivl7qlX0dz"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xbGqOmtMw4NR",
      "metadata": {
        "id": "xbGqOmtMw4NR"
      },
      "source": [
        "\n",
        "* GPU 확보\n",
        "    * Google Colab Pro: V100 또는 P100 GPU가 할당\n",
        "* GPU 확보 방법\n",
        "    * 런타임 -> 런타임 유형 변경\n",
        "    * None -> GPU 변경\n",
        "* GPU 할당 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2kBtM9XSjKE5",
      "metadata": {
        "id": "2kBtM9XSjKE5",
        "outputId": "3b93497c-4c08-4c17-c23e-3625dab455e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr 26 09:50:49 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   49C    P0             33W /  135W |       0MiB /   8192MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6WwnavbBuezQ",
      "metadata": {
        "id": "6WwnavbBuezQ"
      },
      "source": [
        "Colab에서 제공하는 GPU사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1da5fff",
      "metadata": {
        "id": "e1da5fff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a528c1a",
      "metadata": {
        "id": "8a528c1a"
      },
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "\n",
        "* 학습 중 모델 체크포인트를 직접 Hugging Face Hub에 업로드하는 것을 강력히 권장\n",
        " * Hub를 사용하면 다음과 같은 기능을 제공:\n",
        "\n",
        " 1. 통합된 버전 관리: 훈련 중에 어떠한 모델 체크포인트도 손실되지 않음\n",
        " 2. Tensorboard 로그: 훈련 과정에서 중요한 지표를 추적\n",
        " 3. 모델 카드: 모델이 무엇을 하고 의도된 사용 사례를 문서화\n",
        " 4. 커뮤니티: 커뮤니티와 쉽게 공유하고 협업\n",
        "\n",
        "* Hub에 연결\n",
        " * 프롬프트에서 Hub 인증 토큰을 입력:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0OpduhX2JF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8dcb31625fff4f7da6d28821ef535958",
            "de61db8206fe4a6da225263fe67d6fda",
            "33b8358abdd84d2db2a676cd78e34e24",
            "9f7069bb8321494eb6d8184eade793f3",
            "49d1a842d5374d7d90223a895dad23a1",
            "ee3a40d801654eb7957204287584baca",
            "11ebc9c8a98a471fae03bbf78f266017",
            "eb5347528f2440279d88be725e435df7",
            "ee04cb67adc24431b9279db5dbfcd233",
            "e96669dc833348558eaba0a863000992",
            "32268d5cfdbf4cada7ecf99f500d78e3",
            "06292a5c7aee497da4adf4bad7821023",
            "d278e3ffc1504e5790aaf4250bbdc0fb",
            "d7f2b46662614e7a86d5bb40ec8dfbca",
            "4d28829ac5fd43c0978fb4abdba1232a",
            "66b90e6bde694e819da181fc0536daf7",
            "948d115a19b446cd9f75a7e5b8210571",
            "74bfe188e1004755b580367771a08150",
            "1c6783f29c6d4473a1f2f6936fa8682a",
            "323d4ce1247e47ffa4d7402ae9d56187",
            "e6ad995b0183445eb165aff824a649fa",
            "8d7c7ef49aec459783389b840fe3a5c6",
            "2e39340a463a4a36aa5b89c468443340",
            "0941f6a14ce44d828a76bba404be355f"
          ]
        },
        "id": "ed0OpduhX2JF",
        "outputId": "ad965bd7-974f-4621-c149-013ca660095f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0941f6a14ce44d828a76bba404be355f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gG-2lYDPw3uW",
      "metadata": {
        "id": "gG-2lYDPw3uW"
      },
      "source": [
        "\n",
        "Whisper 모델 checkpoint와 task 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJ9M1WKhu0KM",
      "metadata": {
        "id": "mJ9M1WKhu0KM"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"openai/whisper-large-v2\"\n",
        "task = \"transcribe\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EuhppXc9xAt2",
      "metadata": {
        "id": "EuhppXc9xAt2"
      },
      "source": [
        "\n",
        "\n",
        "데이터셋 디테일을 설정 (언어)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7sE0FPf7w-he",
      "metadata": {
        "id": "7sE0FPf7w-he"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"mozilla-foundation/common_voice_13_0\"\n",
        "language = \"Korean\"\n",
        "language_abbr = \"ko\" # Short hand code for the language we want to fine-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XPI5OZz4u3b1",
      "metadata": {
        "id": "XPI5OZz4u3b1"
      },
      "source": [
        "# 데이터셋 올리기\n",
        "\n",
        "* Huggingface Dataset 사용\n",
        " * 적은 코드로 Common Voice의 데이터를 다운로드하고 준비\n",
        "\n",
        "* 확인 절차\n",
        " 1. Hugging Face Hub의 이용 약관을 수락했는지 확인:[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)\n",
        " 2. 데이터셋에 액세스하고 로컬로 데이터를 다운로드\n",
        "\n",
        "* 학습+검증 데이터셋/테스트 데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
        "outputId": "2d9c0a24-e1f9-40eb-c8be-67b421e4431a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 297\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 131\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", token=True)\n",
        "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", token=True)\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805b1c56",
      "metadata": {
        "id": "805b1c56"
      },
      "source": [
        "* 일반적인 ASR(음성 인식) 데이터셋\n",
        "    * 입력 오디오 샘플(오디오)과 해당되는 텍스트(문장)만 제공\n",
        "* Common Voice\n",
        "    * ASR에는 필요하지 않은 악센트와 로케일과 같은 추가 메타데이터 정보가 포함\n",
        "    * 일반적인 용도로 사용하고 미세 조정을 고려하기 위해 메타데이터 정보 무시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
      "metadata": {
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
        "outputId": "2b426cc2-57db-4af9-ffd6-786020ed96a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 297\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 131\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "common_voice = common_voice.remove_columns(\n",
        "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
      "metadata": {
        "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
      },
      "source": [
        "## 특성 추출기(Feature Extractor), 토크나이저(Tokenizer), 그리고 데이터준비\n",
        "\n",
        "ASR 파이프라인 세 단계로 분해:\n",
        "\n",
        "1. Raw 오디오 입력을 전처리하는 특정 추출기\n",
        "2. 시퀀스 간 매핑을 수행하는 모델\n",
        "3. 모델 출력을 텍스트 형식으로 후처리하는 tokenizer\n",
        "\n",
        "\n",
        "* Whisper\n",
        "    * [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)와 [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)로 구성\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "6089bc99-7131-4638-c1fc-c9132971d2d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gIaGxWbXkcrC",
      "metadata": {
        "id": "gIaGxWbXkcrC"
      },
      "source": [
        "* WhisperProcessor 클라스\n",
        "    * 특성 추출기와 토크나이저를 사용하기 위해 두 가지를 모두 합칩\n",
        "    * 필요에 따라 오디오 입력 및 모델 예측에 사용 가능\n",
        "\n",
        "* 학습 중에 두 개의 객체만 추적 필요: 프로세서와 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
        "outputId": "71f52347-9942-4e6c-d5f7-305cc0862127"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### 데이터 준비\n",
        "\n",
        "Common Voice 데이터셋의 첫 번째 예제를 출력하여 데이터의 형식을 살펴봄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
      "metadata": {
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
        "outputId": "902f73dc-35ff-4289-b420-00c2bd8bddf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\f4cce98295bb5b9d65d17d7d495bef59577e50b412400821477bb56f2c098300\\\\ko_train_0/common_voice_ko_35845802.mp3', 'array': array([-4.54747351e-13,  4.77484718e-12,  6.59383659e-12, ...,\n",
            "        2.07564299e-05,  1.92765128e-06, -1.09442644e-05]), 'sampling_rate': 48000}, 'sentence': '어느덧 그 더운 팔월도 하루를 남기고 다 지나 버렸다.'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
      "metadata": {
        "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
      },
      "source": [
        "* Whisper 모델 샘플링\n",
        "    * 입력 오디오는 48 kHz 새플링\n",
        "    * Whisper feature extractor에 전달하기 위해서 16 kHz로 다운샘플 진행\n",
        "* 샘플링 속도 설정\n",
        "    * Dataset의 [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) 방법 사용: 오디오 입력을 올바른 샘플링 속도로 설정\n",
        "    * 오디오를 변경하는 것이 아니라 오디오 샘플을 실시간으로 받을 수 있도록 함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
      "metadata": {
        "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
      },
      "source": [
        "\n",
        "Common Voice 데이터셋에서 첫 번째 오디오 샘플을 다시로드하면 원하는 샘플링 속도로 다시 샘플링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87122d71-289a-466a-afcf-fa354b18946b",
      "metadata": {
        "id": "87122d71-289a-466a-afcf-fa354b18946b",
        "outputId": "ee834319-5f0c-44f6-cae8-80d2741a49b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\f4cce98295bb5b9d65d17d7d495bef59577e50b412400821477bb56f2c098300\\\\ko_train_0/common_voice_ko_35845802.mp3', 'array': array([ 1.45519152e-11,  0.00000000e+00,  2.18278728e-11, ...,\n",
            "        4.60762531e-05,  3.03988345e-05, -1.03190541e-06]), 'sampling_rate': 16000}, 'sentence': '어느덧 그 더운 팔월도 하루를 남기고 다 지나 버렸다.'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
      "metadata": {
        "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
      },
      "source": [
        "\n",
        "* 모델에 맞게 데이터를 준비하는 함수:\n",
        "\n",
        "1. batch[\"audio\"]를 호출하여 오디오 데이터를 로드하고 다시 샘플링. 🤗 Datasets는 필요한 모든 재샘플링 작업을 실시간으로 수행\n",
        "3. Feature extractor를 사용하여 1차원 오디오 배열에서 로그 멜 스펙트로그램 입력 특성을 계산\n",
        "3. Tokenizer를 사용하여 transcripts를 레이블 ids로 인코딩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "# def prepare_dataset(batch):\n",
        "#     # load and resample audio data from 48 to 16kHz\n",
        "#     audio = batch[\"audio\"]\n",
        "\n",
        "#     # compute log-Mel input features from input audio array\n",
        "#     batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "#     # encode target text to label ids\n",
        "#     batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "#     return batch\n",
        "\n",
        "def prepare_dataset(batch, feature_extractor, tokenizer):\n",
        "    # Load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # Compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # Encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
      "metadata": {
        "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
      },
      "source": [
        "* 데이터 준비 함수\n",
        "    * 데이터셋의 .map 메서드를 사용하여 모든 학습 예제에 적용 가능\n",
        "    * num_proc: 몇 개의 CPU 코어를 사용할 지를 지정,num_proc를 1보다 크게 설정하면 다중 처리가 활성화 (다중 처리로 .map 메서드가 중단되는 경우 num_proc=1로 설정하고 데이터셋을 순차적으로 처리)\n",
        "\n",
        "* Dataset의 사이즈에 따라서 20~30 정도 걸림\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
        "scrolled": true,
        "colab": {
          "referenced_widgets": [
            "ad6d83e9d32f44448d077a1685934cb9",
            "d9271509fb2a4562a7a83fb91039e4f4"
          ]
        },
        "outputId": "09d78a37-ae94-44f2-85c8-33f81fea2720"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad6d83e9d32f44448d077a1685934cb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/297 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9271509fb2a4562a7a83fb91039e4f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/131 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)\n",
        "\n",
        "common_voice = common_voice.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=common_voice.column_names[\"train\"],\n",
        "    num_proc=2,\n",
        "    fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4be572c",
      "metadata": {
        "id": "c4be572c",
        "outputId": "27d5006e-dff0-4cd9-98e4-7f4e9b4dbc56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_features', 'labels'],\n",
              "    num_rows: 297\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
      "metadata": {
        "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
      },
      "source": [
        "## 학습 및 검증\n",
        "\n",
        "\n",
        "* 훈련 파이프라인\n",
        "* [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)가 대부분의 작업을 처리:\n",
        "\n",
        "\n",
        "1. 데이터 collator 정의: 데이터 콜레이터는 우리가 전처리한 데이터를 가져와 모델에 사용할 수 있는 PyTorch 텐서로 준비\n",
        "2. 평가 지표: 평가 중에는 모델을 글자 오류율  [word error rate (CER)](https://huggingface.co/metrics/cer)지표를 사용하여 평가\n",
        "3. 사전 훈련된 체크포인트 load: 사전 훈련된 체크포인트를 로드하고 훈련을 위해 올바르게 구성\n",
        "4. 훈련 구성 정의: 🤗 Trainer가 훈련 스케줄을 정의에 사용\n",
        "\n",
        "* 미세 조정한 후에는 테스트 데이터에서 모델을 평가하여 한국어 음성을 올바르게 transcribe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Data Collator 정의\n",
        "\n",
        "* 시퀀스-투-시퀀스 음성 모델의 데이터 콜레이터\n",
        "    * Input_features와 labels를 독립적으로 처리 차별화\n",
        "    \n",
        "    - Input_features: feature extractor에 의해 처리\n",
        "    - labels: tokenizer에 의해 처리\n",
        "\n",
        "* Input_features는 이미 30초로 패딩되어 있고 특성 추출기에 의해 고정된 차원의 로그 멜 스펙트로그램으로 변환. 따라서 우리가 해야 할 일은 input_features를 배치 처리된 PyTorch 텐서로 변환\n",
        "\n",
        "* labels는 패딩되지 않음 먼저 배치 내에서 최대 길이에 맞게 시퀀스를 패딩하고, tokenizer의 .pad 메서드를 사용하여 시퀀스를 패딩 패딩 토큰은 손실을 계산할 때 고려되지 않도록 -100으로 대체. 그런 다음 레이블 시퀀스의 시작에서 BOS 토큰을 잘라서 훈련 중에 나중에 이를 추가\n",
        "\n",
        "* 이전에 정의한 WhisperProcessor를 활용하여 특성 추출기 및 토크나이저 작업을 모두 수행 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "Data collator 초기화 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### 평가 지표"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fee1a7-a44c-461e-b047-c3917221572e",
      "metadata": {
        "id": "66fee1a7-a44c-461e-b047-c3917221572e"
      },
      "source": [
        "\n",
        "* ASR 시스템을 평가하기 위한 '사실상의' 지표인 한 단어 오류율(CER) 메트릭을 사용\n",
        "* 더 많은 정보는 [문서](https://huggingface.co/metrics/cer)를 참조. 우리는 🤗 Evaluate에서 CER 메트릭을 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890",
        "colab": {
          "referenced_widgets": [
            "95a681cad8b642fd93b43022f837f63d"
          ]
        },
        "outputId": "2238dd6d-96b3-437a-a9de-6f5e84bbd1fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95a681cad8b642fd93b43022f837f63d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"cer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf2a825-6d9f-4a23-b145-c37c0039075b",
      "metadata": {
        "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
      },
      "source": [
        "### Pre-trained 모델 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
      "metadata": {
        "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
      },
      "source": [
        "* 사전 훈련된 Whisper 체크포인트를 로드\n",
        "    * 이 작업은 🤗 Transformers를 사용하여 매우 간단\n",
        "\n",
        "\n",
        "\n",
        "* 모델의 메모리 사용량을 줄이기 위해 모델을 8비트로         \n",
        "    * 모델을 1/4 정밀도(32비트와 비교했을 때)로 양자화하여 성능 손실을 최소화 [here](https://huggingface.co/blog/hf-bitsandbytes-integration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
        "outputId": "f9f1c990-f05d-48dc-a211-505bb0060ff6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map={\"\":0})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bR-_yaEOPsfQ",
      "metadata": {
        "id": "bR-_yaEOPsfQ"
      },
      "source": [
        "### 모델의 후처리\n",
        "\n",
        "1. 훈련을 가능하게 하기 위해 8비트 모델에 몇 가지 후처리 단계를 적용\n",
        "2. 모델 레이어를 동결, 훈련과 모델의 안정성을 위해 레이어 정규화와 출력 레이어를 float32로 캐스팅\n",
        "\n",
        "(모델 안정성과 layer normalization 분석, float32로 캐스팅 하는 이유)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cl_ZQualPt9R",
      "metadata": {
        "id": "Cl_ZQualPt9R",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0Ja2e__OX02",
      "metadata": {
        "id": "p0Ja2e__OX02"
      },
      "source": [
        "* Whisper 모델은 인코더에 컨볼루션 레이어를 사용하기 때문에 체크포인팅은 grad 연산을 비활성. 이를 피하기 위해 입력을 특별히 trainable하게 만들어야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmpeiajSOWCy",
      "metadata": {
        "id": "bmpeiajSOWCy",
        "outputId": "94947ae5-4739-4547-a8ae-3d9fe75c461a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x1d1d8658520>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vjl4j4RJPmPR",
      "metadata": {
        "id": "Vjl4j4RJPmPR"
      },
      "source": [
        "### Low-rank adapters (LoRA)를 모델에 적용\n",
        "\n",
        "* peft에서 get_peft_model 유틸리티 함수를 사용하여 PeftModel을 로드하고 저희가 저차원 어댑터(LoRA)를 사용할 것임을 지정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DQtpDPRHPyOL",
      "metadata": {
        "id": "DQtpDPRHPyOL",
        "outputId": "eee9345a-bb81-402a-8048-c3aacf8d843f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3906d436",
      "metadata": {
        "id": "3906d436"
      },
      "source": [
        "**1%**의 학습 parameter를 사용하였고 **Parameter-Efficient Fine-Tuning**를 적용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### 훈련 구성 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "source": [
        "\n",
        "마지막 단계에서는 훈련과 관련된 모든 매개변수를 정의 훈련 인자에 대한 자세한 내용은 해당 문서를 참조 Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a944d8-3112-4552-82a0-be25988b3857",
      "metadata": {
        "id": "b3a944d8-3112-4552-82a0-be25988b3857"
      },
      "source": [
        "\n",
        "* PEFT를 사용하여 모델을 미세 조정하는 것에는 몇 가지 주의가 필요\n",
        "\n",
        "1. PeftModel의 forward가 기본 모델의 forward의 시그니처를 상속하지 않기 때문에 remove_unused_columns=False 및 label_names=[\"labels\"]를 명시적으로 설정\n",
        "2.INT8 훈련에는 자동 캐스팅이 필요하기 때문에 Trainer에서 기본적으로 제공되는 predict_with_generate 호출을 사용할 수 없습니다. 자동 캐스팅이 자동으로 적용되지 않음\n",
        "3. 자동 캐스팅을 사용할 수 없으므로 Seq2SeqTrainer에 compute_metrics를 전달할 수 없음. 따라서 Trainer를 인스턴스화하는 동안 compute_metrics를 주석 처리 필요\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493",
        "outputId": "f24abc46-f71d-411d-e716-ea3190ab42e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
        "outputId": "a469629e-eb68-47de-dc61-5c1a659a6191"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\AEO\\anaconda3\\envs\\gpuconda\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "C:\\AEO\\anaconda3\\envs\\gpuconda\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "C:\\AEO\\anaconda3\\envs\\gpuconda\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:694: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 16:42, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.541300</td>\n",
              "      <td>0.231048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.5412639236450195, metrics={'train_runtime': 1017.2357, 'train_samples_per_second': 0.786, 'train_steps_per_second': 0.098, 'total_flos': 1.6866147262464e+18, 'train_loss': 0.5412639236450195, 'epoch': 2.6315789473684212})"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8iqXhUiuBQCs",
      "metadata": {
        "id": "8iqXhUiuBQCs"
      },
      "source": [
        "Fine-tuning한 모델을 Hugging Face Hub에 저장합니다. 나중에 모델을 불러올 때 편합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0576aa2a",
      "metadata": {
        "id": "0576aa2a",
        "colab": {
          "referenced_widgets": [
            "07b05185fc8c4d51a9be1f16e095d33c"
          ]
        },
        "outputId": "29072fbc-f838-4776-c37f-56ff92bd2cd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b05185fc8c4d51a9be1f16e095d33c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/63.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/ZeroWater93/whisper-large-v2-korea-common_13/commit/71992e7d98bf7aaa1135f11a84742d63d4b80df3', commit_message='Upload model', commit_description='', oid='71992e7d98bf7aaa1135f11a84742d63d4b80df3', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model_id = \"ZeroWater93/whisper-large-v2-korea-common_13\"\n",
        "model.push_to_hub(peft_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SlyyOGnPgi_I",
      "metadata": {
        "id": "SlyyOGnPgi_I"
      },
      "source": [
        "# 평가 및 검증"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kzfg2qoXgrhg",
      "metadata": {
        "id": "Kzfg2qoXgrhg"
      },
      "source": [
        "Finetuning을 성공적으로 했으면 이제 테스트 데이터셋에서 저희 모델을 테스트하고 CER(Character Error Rate) 계산\n",
        "\n",
        "테스트 유의할 점들:\n",
        "\n",
        "1. predict_with_generate 함수를 사용할 수 없으므로 자체적으로 torch.cuda.amp.autocast()를 사용하여 eval 루프를 구현\n",
        "\n",
        "2. 기본 모델이 동결되어 있기 때문에 PEFT 모델은 때로 디코딩 중에 언어를 인식하지 못할 수 있음. 이를 해결하기 위해 디코딩 시작 토큰에 번역 중인 언어를 명시적으로 지정. 이 작업은 forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")를 사용하여 수행하고 model.generate 호출에 이를 전달\n",
        "\n",
        "Transcribe를 진행 🔥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "273a996c",
      "metadata": {
        "id": "273a996c",
        "scrolled": true,
        "colab": {
          "referenced_widgets": [
            "5fe5cae6805543f78b70a88555eee745"
          ]
        },
        "outputId": "5d69a065-98b6-4fd6-b708-8b51b61f9216"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fe5cae6805543f78b70a88555eee745",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m peft_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZeroWater93/whisper-large-v2-korea-common_13\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Use the same model ID as before.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_model_id)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, peft_model_id)\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mC:\\AEO\\anaconda3\\envs\\gpuconda\\lib\\site-packages\\transformers\\modeling_utils.py:3627\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3624\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3627\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3630\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[1;32mC:\\AEO\\anaconda3\\envs\\gpuconda\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:86\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[0;32m     84\u001b[0m     }\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m            Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m            quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m            in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m            `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m            https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m            for more details.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m            \"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.37.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "
          ]
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"ZeroWater93/whisper-large-v2-korea-common_13\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401ceaa6",
      "metadata": {
        "id": "401ceaa6",
        "outputId": "ab751ca3-fc9f-4bca-95c1-761f4b96773e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|█████████████████████████████▎                                                     | 6/17 [01:37<02:59, 16.35s/it]"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j3XF0PzsCV0v",
      "metadata": {
        "id": "j3XF0PzsCV0v"
      },
      "source": [
        "## 마무리!\n",
        "\n",
        "Whisper 체크포인트를 더 빠르고 저렴하게 훈련하고 CER에서 거의 손실이 없도록 학습하는 방법을 배움\n",
        "\n",
        "PEFT (Pretraining Efficiently with Fine-Tuning)를 사용하면 음성 인식 이외에도 다른 사전 훈련된 모델에 동일한 기술 세트를 적용 가능. 아래 링크에서 자세히 설명: https://github.com/huggingface/peft 🤗"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06292a5c7aee497da4adf4bad7821023": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11ebc9c8a98a471fae03bbf78f266017": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1c6783f29c6d4473a1f2f6936fa8682a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e39340a463a4a36aa5b89c468443340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32268d5cfdbf4cada7ecf99f500d78e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "323d4ce1247e47ffa4d7402ae9d56187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b8358abdd84d2db2a676cd78e34e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e96669dc833348558eaba0a863000992",
            "placeholder": "​",
            "style": "IPY_MODEL_32268d5cfdbf4cada7ecf99f500d78e3",
            "value": ""
          }
        },
        "49d1a842d5374d7d90223a895dad23a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d7f2b46662614e7a86d5bb40ec8dfbca",
            "style": "IPY_MODEL_4d28829ac5fd43c0978fb4abdba1232a",
            "tooltip": ""
          }
        },
        "4d28829ac5fd43c0978fb4abdba1232a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "66b90e6bde694e819da181fc0536daf7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74bfe188e1004755b580367771a08150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6783f29c6d4473a1f2f6936fa8682a",
            "placeholder": "​",
            "style": "IPY_MODEL_323d4ce1247e47ffa4d7402ae9d56187",
            "value": "Connecting..."
          }
        },
        "8d7c7ef49aec459783389b840fe3a5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dcb31625fff4f7da6d28821ef535958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6ad995b0183445eb165aff824a649fa"
            ],
            "layout": "IPY_MODEL_11ebc9c8a98a471fae03bbf78f266017"
          }
        },
        "948d115a19b446cd9f75a7e5b8210571": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f7069bb8321494eb6d8184eade793f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_06292a5c7aee497da4adf4bad7821023",
            "style": "IPY_MODEL_d278e3ffc1504e5790aaf4250bbdc0fb",
            "value": true
          }
        },
        "d278e3ffc1504e5790aaf4250bbdc0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f2b46662614e7a86d5bb40ec8dfbca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de61db8206fe4a6da225263fe67d6fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb5347528f2440279d88be725e435df7",
            "placeholder": "​",
            "style": "IPY_MODEL_ee04cb67adc24431b9279db5dbfcd233",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e6ad995b0183445eb165aff824a649fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d7c7ef49aec459783389b840fe3a5c6",
            "placeholder": "​",
            "style": "IPY_MODEL_2e39340a463a4a36aa5b89c468443340",
            "value": "Invalid token passed!"
          }
        },
        "e96669dc833348558eaba0a863000992": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb5347528f2440279d88be725e435df7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee04cb67adc24431b9279db5dbfcd233": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee3a40d801654eb7957204287584baca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b90e6bde694e819da181fc0536daf7",
            "placeholder": "​",
            "style": "IPY_MODEL_948d115a19b446cd9f75a7e5b8210571",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
